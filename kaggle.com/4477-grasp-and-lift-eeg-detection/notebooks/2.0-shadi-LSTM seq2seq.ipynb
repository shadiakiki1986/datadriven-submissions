{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take at EEG - hand motion data\n",
    "\n",
    "Will try to fit to first subject's data only\n",
    "\n",
    "- training on series 1-6, testing on series 7-8\n",
    "  - downsampled every 10 points for faster computation\n",
    "  - on gpu\n",
    "    - Epoch 169/3000    - 2s    - loss: 0.0494\n",
    "        - reconstructed_features_loss: 0.0445\n",
    "        - reconstructed_targets_loss: 0.0037\n",
    "        - regressed_output_loss: 0.0012\n",
    "    - the above is for batch-size = 2**14 ~ 16'000 and loss = MAE\n",
    "    - Epoch 1103/3000 - 2s - loss: 0.0383\n",
    "        - reconstructed_features_loss: 0.0361\n",
    "        - reconstructed_targets_loss: 0.0018\n",
    "        - regressed_output_loss: 3.5944e-04\n",
    "\n",
    "    - TODO check if better to just use multi-core with smaller batch-size ~ 32\n",
    "\n",
    "- TODO is downsampling harmful for training/prediction?\n",
    "  - downsample_pts = 1 kills the kernel\n",
    "  \n",
    "- TODO should the min/max scaling be per series?\n",
    "- TODO lahead is currently in \"points\". So needs to be changed depending on downsampling.\n",
    "- TODO model \"subtract\" output is measured as squared error (with target = zeros)\n",
    "  - can I do better? something like \"binary cross-entropy\"?\n",
    "- TODO does AE on target make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check gpu usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_train: number of points for training, as opposed to testing\n",
    "# lahead: stride data with \"lahead\" window size\n",
    "# batch_size: keras.model.fit parameter .. smaller batches lead to less loss of data when truncating non-multiples of batch_size\n",
    "# downsample_pts: 1 for no downsampling, 10 for downsample by 10\n",
    "#---------------------------------------------------------\n",
    "# set 1\n",
    "# n_train, lahead, batch_size, downsample_pts = 120000, 10, 2**14, 10\n",
    "\n",
    "# set 2\n",
    "n_train, lahead, batch_size, downsample_pts = 1200000, 100, 2**18, 1\n",
    "\n",
    "# set 3:\n",
    "# training each subject / series separately\n",
    "# Requires smaller batch_size since each series is only around 1000 pts when downsampled by 10\n",
    "# n_train, lahead, batch_size, downsample_pts = 120000, 10, 2**4, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Dropout, Embedding, Flatten, Subtract, Dot, Activation\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, RepeatVector, TimeDistributed, Concatenate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load(subj_ids:list, series_ids:list):\n",
    "    features_all = []\n",
    "    targets_all = []\n",
    "    for i1 in subj_ids:\n",
    "        for i2 in series_ids:\n",
    "            for i3, fn in [\n",
    "                ('features', 'data/raw/train/subj%i_series%i_data.csv'%(i1, i2)),\n",
    "                ('targets', 'data/raw/train/subj%i_series%i_events.csv'%(i1, i2)),\n",
    "            ]:\n",
    "                print('status', i1, i2, i3)\n",
    "                xxx_i = pd.read_csv(fn)\n",
    "                xxx_i['subj_id'] = i1\n",
    "                xxx_i['series_id'] = i2\n",
    "                xxx_i = xxx_i.set_index(['subj_id', 'series_id', 'id']).astype('int16')\n",
    "                xxx_i = xxx_i[::downsample_pts] # downsample\n",
    "                if i3=='features':\n",
    "                    features_all.append(xxx_i)\n",
    "                else:\n",
    "                    targets_all.append(xxx_i)\n",
    "            \n",
    "    features_all = pd.concat(features_all, axis=0)\n",
    "    targets_all = pd.concat(targets_all, axis=0)\n",
    "    return features_all, targets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets = my_load(subj_ids = [1], series_ids = [x+1 for x in range(8)])\n",
    "train_features.shape, train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess features\n",
    "\n",
    "e.g. scale to [0,1], stride, truncate, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_df(df, n_back):\n",
    "    \"\"\"\n",
    "    create rolling windows for LSTM\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(n_back):\n",
    "        out.append(df.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[(n_back-1):, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    out = np.flip(out, axis=1) # so that the index=0 is the oldest, and index=4 is latest\n",
    "    return out\n",
    "\n",
    "stride_df_2 = lambda x: stride_df(x, lahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_truncate(df):\n",
    "    \"\"\"\n",
    "    drop 1st x rows if they are not a multiple of batch_size\n",
    "    \"\"\"\n",
    "    return df.tail(df.shape[0] - (df.shape[0]%batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_train, y_train):\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    def wrap_pd_df(xxx, func):\n",
    "        return pd.DataFrame(\n",
    "                 func(xxx), \n",
    "                 columns=xxx.columns, \n",
    "                 index=xxx.index\n",
    "               )\n",
    "\n",
    "    print('min/max start')\n",
    "    # xtrain_pre = x_train.groupby(['subj_id', 'series_id']).apply(lambda xxx: scaler.fit_transform(xxx))\n",
    "    xtrain_pre = ( x_train.groupby(['subj_id', 'series_id'])\n",
    "                          .apply(lambda xxx: wrap_pd_df(xxx, lambda yyy: scaler.fit_transform(yyy)))\n",
    "                 )\n",
    "    ytrain_pre = y_train\n",
    "\n",
    "    print('train_pre', xtrain_pre.shape, ytrain_pre.shape)\n",
    "    #--------------------------------------\n",
    "    # xtrain_roll = stride_df_2(xtrain_pre)\n",
    "    # ytrain_roll = stride_df_2(ytrain_pre)\n",
    "    xtrain_roll = (xtrain_pre.groupby(['subj_id', 'series_id'])\n",
    "                             .apply(stride_df_2)\n",
    "                             # .apply(lambda xxx: wrap_pd_df(xxx, stride_df_2))\n",
    "                  )\n",
    "    ytrain_roll = (ytrain_pre.groupby(['subj_id', 'series_id'])\n",
    "                             .apply(stride_df_2)\n",
    "                             # .apply(lambda xxx: wrap_pd_df(xxx, stride_df_2))\n",
    "                  )\n",
    "\n",
    "    # \"meta\" dataframe that will still contain the pandas index (above *_roll variables are numpy matrices)\n",
    "    ztrain_roll = y_train.groupby(['subj_id', 'series_id']).apply(lambda group: group.iloc[(lahead-1):])\n",
    "\n",
    "    print('train_roll 1', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    #return xtrain_roll, ytrain_roll, ztrain_roll\n",
    "\n",
    "    \"\"\"\n",
    "    # drop non-batchsize-multiple per subject/series pair\n",
    "    for (subj_id, series_id), group in xtrain_roll.groupby(['subj_id', 'series_id']):\n",
    "        to_drop = group.values[0].shape[0] % batch_size\n",
    "        print(subj_id, series_id, 'drop non-multiple', to_drop)\n",
    "        assert to_drop < 1000\n",
    "\n",
    "        xtrain_roll.loc[subj_id, series_id] = xtrain_roll.loc[subj_id, series_id][(to_drop):]\n",
    "        ytrain_roll.loc[subj_id, series_id] = ytrain_roll.loc[subj_id, series_id][(to_drop):]\n",
    "       \n",
    "    ztrain_roll = ztrain_roll.groupby(['subj_id', 'series_id']).apply(my_truncate)\n",
    "    print('train_roll 2', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # aggregate all strided matrices\n",
    "    xtrain_roll = np.concatenate(xtrain_roll.values, axis=0)\n",
    "    ytrain_roll = np.concatenate(ytrain_roll.values, axis=0)\n",
    "    \n",
    "    # drop non-batchsize-multiple, once for all\n",
    "    to_drop = xtrain_roll.shape[0]%batch_size\n",
    "    print('drop non-multiple', to_drop)\n",
    "    xtrain_roll = xtrain_roll[(to_drop):]\n",
    "    ytrain_roll = ytrain_roll[(to_drop):]\n",
    "    ztrain_roll = my_truncate(ztrain_roll)\n",
    "    print('train_roll 2', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    \n",
    "    return xtrain_roll, ytrain_roll, ztrain_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features.head(n=n_train).copy()\n",
    "y_train = train_targets.head(n=n_train).copy()\n",
    "print('x_train, y_train', x_train.shape, y_train.shape)\n",
    "\n",
    "xtrain_roll, ytrain_roll, ztrain_roll = preprocess(x_train, y_train)\n",
    "assert xtrain_roll.shape[0] > 0\n",
    "xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_plot = pd.DataFrame(xtrain_roll[:,-1,:], columns=x_train.columns)\n",
    "print(xtrain_plot.shape)\n",
    "xtrain_plot[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[['HandStart']].head(n=1000).plot(figsize=(20,3), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## avoid class bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate length of HandStart == 1\n",
    "y_temp = y_train['HandStart'].diff().fillna(value=0)\n",
    "y_start = y_temp[y_temp > 0]\n",
    "y_end   = y_temp[y_temp < 0]\n",
    "y_start.head(), y_end.head() # length = 150, at downsample=10 this becomes 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test \"shift\" function\n",
    "# pd.DataFrame({'A': range(5)}).shift(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_mask(mask_in, n_stretch):\n",
    "    \"\"\"\n",
    "    n_stretch = 10 <=> stretch mask by 5 from each side\n",
    "    \"\"\"\n",
    "    i_sig = np.ones(1*n_stretch)\n",
    "    i_sig = np.convolve(mask_in, i_sig, mode='same')\n",
    "    i_sig = i_sig > 0\n",
    "    return i_sig\n",
    "\n",
    "\n",
    "# test stretching methodology\n",
    "i_one = (ytrain_roll[:,-1,0]==1)\n",
    "n_show = 1000\n",
    "for i_iter in [1, 20, 40]:\n",
    "    print(i_iter)\n",
    "    i_sig = stretch_mask(i_one[:n_show], i_iter)\n",
    "    plt.plot(i_one[:n_show], label='ori')\n",
    "    plt.plot(i_sig, label='stretched')\n",
    "    plt.title(i_iter)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handstart_len = 15 # from above\n",
    "#\n",
    "# i_one = (ytrain_roll[:,-1,0]==1)\n",
    "# i_pree = pd.DataFrame(ytrain_roll[:,-1,:1]).shift(-1*handstart_len//2).fillna(value=0)==1\n",
    "# i_post = pd.DataFrame(ytrain_roll[:,-1,:1]).shift(+1*handstart_len//2).fillna(value=0)==1\n",
    "# mask = i_one | i_pree.values.squeeze() | i_post.values.squeeze()\n",
    "\n",
    "mask = (ytrain_roll[:,-1,0]==1)\n",
    "mask = stretch_mask(mask, 40)\n",
    "\n",
    "xtrain_bal = xtrain_roll[mask]\n",
    "ytrain_bal = ytrain_roll[mask]\n",
    "ztrain_bal = ztrain_roll[mask]\n",
    "\n",
    "xtrain_bal.shape, ytrain_bal.shape, ztrain_bal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_plot = pd.DataFrame(xtrain_bal[:,-1,:], columns=x_train.columns)\n",
    "ytrain_plot = pd.DataFrame(ytrain_bal[:,-1,:], columns=y_train.columns)\n",
    "print(xtrain_plot.shape)\n",
    "xtrain_plot[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "ytrain_plot[['HandStart']].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model: AE coupled with regression on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coupled():\n",
    "    lstm_dim_1 = 15\n",
    "    len_feat = xtrain_roll.shape[2]\n",
    "    len_targ = 1\n",
    "    input_shape = (lahead, len_feat, )\n",
    "\n",
    "    # features encoder\n",
    "    feat_raw = Input(shape=input_shape, name='raw_features')\n",
    "    feat_enc = feat_raw\n",
    "    feat_enc = LSTM(\n",
    "              lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=False,\n",
    "              activation='tanh',\n",
    "              name='encoded_features')(feat_enc)\n",
    "\n",
    "    # features decoder\n",
    "    targ_rec = feat_enc\n",
    "    targ_rec = RepeatVector(lahead, input_shape=(lstm_dim_1, ))(targ_rec)\n",
    "    targ_rec = LSTM(lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=True,\n",
    "              dropout=0.2,\n",
    "              activation='tanh')(targ_rec)\n",
    "    targ_rec = TimeDistributed(\n",
    "        # Dense(len_targ, activation='linear'),\n",
    "        Dense(len_targ, activation='sigmoid'),\n",
    "        name='reconstructed_targets'\n",
    "    )(targ_rec)\n",
    "\n",
    "    # create model\n",
    "    model_all = Model(inputs = [feat_raw], outputs = [targ_rec])\n",
    "    return model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "def double_binary_crossentropy(y_true, y_pred):\n",
    "    return K.mean(binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "mod2 = create_coupled()\n",
    "# mod2.compile(loss='mae', optimizer='adam')\n",
    "mod2.compile(loss=double_binary_crossentropy, optimizer='adam')\n",
    "# mod2.compile(loss=double_binary_crossentropy, optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "mod2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(model, np_in, index):\n",
    "    \n",
    "    # make prediction\n",
    "    targ_rec = model.predict(np_in, batch_size=batch_size)\n",
    "        \n",
    "    # plot target reconstruction\n",
    "    feat_int = 0\n",
    "    pd.DataFrame({\n",
    "        'actual': pd.Series(np_in['raw_targets'][:,-1,feat_int],  index=index).astype('int16'),\n",
    "        'pred': pd.Series(targ_rec[:,-1,feat_int],  index=index),\n",
    "    }).plot(figsize=(20,3), alpha=0.5)\n",
    "    plt.title('target %i'%(feat_int))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # prepare output\n",
    "    out = pd.DataFrame({\n",
    "        'prediction': targ_rec[:,-1,0].squeeze(), \n",
    "        'id': index,\n",
    "    }).set_index(['id'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train directly on full data, with assigning class_weight in model.fit\n",
    "print(time.ctime(),'fit start')\n",
    "history = mod2.fit(\n",
    "         {   'raw_features': xtrain_roll,\n",
    "         },\n",
    "         {   'reconstructed_targets': ytrain_roll[:,:,:1],\n",
    "         },\n",
    "         batch_size=batch_size,\n",
    "         epochs=150,\n",
    "         # initial_epoch = 17,\n",
    "         verbose=2,\n",
    "         #validation_data=None,\n",
    "         # class_weight = {0: 0.1, 1: 0.9}, # class_weight not supported for 3+ dimensional targets\n",
    "         sample_weight = (ytrain_roll[:,:,0]==1)*0.8 + 0.1,\n",
    "         shuffle=False\n",
    "    )\n",
    "print(time.ctime(),'fit end')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# construct wider masks of the HandStart = 1 and append in single matrix for training\n",
    "mask_init = (ytrain_roll[:,-1,0]==1)\n",
    "xtrain_bal = []\n",
    "ytrain_bal = []\n",
    "ztrain_bal = []\n",
    "for n_stretch in [20, 40, 60, 80, 100, 150, 200, 250, 300]:\n",
    "    print('status', n_stretch)\n",
    "    mask_stretched = stretch_mask(mask_init, n_stretch)\n",
    "\n",
    "    xtrain_bal.append(xtrain_roll[mask_stretched])\n",
    "    ytrain_bal.append(ytrain_roll[mask_stretched])\n",
    "    ztrain_bal.append(ztrain_roll[mask_stretched])\n",
    "    \n",
    "xtrain_bal = np.concatenate(xtrain_bal, axis=0)\n",
    "ytrain_bal = np.concatenate(ytrain_bal, axis=0)\n",
    "ztrain_bal = np.concatenate(ztrain_bal, axis=0)\n",
    "\n",
    "print('shape', xtrain_bal.shape, ytrain_bal.shape, ztrain_bal.shape)\n",
    "\n",
    "print(time.ctime(),'fit start')\n",
    "history = mod2.fit(\n",
    "         {   'raw_features': xtrain_bal,\n",
    "         },\n",
    "         {   'reconstructed_targets': ytrain_bal[:,:,:1],\n",
    "         },\n",
    "         batch_size=batch_size,\n",
    "         epochs=150,\n",
    "         # initial_epoch = 17,\n",
    "         verbose=2,\n",
    "         #validation_data=None,\n",
    "         shuffle=False\n",
    "    )\n",
    "print(time.ctime(),'fit end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over wider masks of the HandStart = 1 and train\n",
    "mask_init = (ytrain_roll[:,-1,0]==1)\n",
    "# for n_stretch in [20, 40, 70, 110, 160, 220, 290, 370]:\n",
    "# for n_stretch in [160, 220, 290, 370]:\n",
    "for n_stretch in [290, 370]:\n",
    "    print('status', n_stretch)\n",
    "    mask_stretched = stretch_mask(mask_init, n_stretch)\n",
    "\n",
    "    xtrain_bal = xtrain_roll[mask_stretched]\n",
    "    ytrain_bal = ytrain_roll[mask_stretched]\n",
    "    ztrain_bal = ztrain_roll[mask_stretched]\n",
    "\n",
    "    print('shape', xtrain_bal.shape, ytrain_bal.shape, ztrain_bal.shape)\n",
    "\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod2.fit(\n",
    "             {   'raw_features': xtrain_bal,\n",
    "             },\n",
    "             {   'reconstructed_targets': ytrain_bal[:,:,:1],\n",
    "             },\n",
    "             batch_size=batch_size,\n",
    "             epochs=250, # FIXME 400?\n",
    "             # initial_epoch = 17,\n",
    "             verbose=2,\n",
    "             #validation_data=None,\n",
    "             validation_split = 0.3,\n",
    "             shuffle=False\n",
    "        )\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    # ignore first few points since large relative to others\n",
    "    # plt.plot(history.history['loss'][5:], label='loss')\n",
    "    plt.plot(history.history['loss'], label='loss') # [5:]\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title('training loss')\n",
    "    plt.show()\n",
    "    \n",
    "    # predict on balanced series (plots implicitly actual vs predicted)\n",
    "    n_show = 1000\n",
    "    ytrain_pred = my_predict(\n",
    "        mod2,\n",
    "        {   'raw_features': xtrain_bal[:n_show],\n",
    "            'raw_targets':  ytrain_bal[:n_show],\n",
    "        },\n",
    "        ztrain_bal.index[:n_show],\n",
    "    )\n",
    "    # ytrain_pred.shape\n",
    "    \n",
    "    # predict on non-balanced series\n",
    "    n_show = 1000\n",
    "    ytrain_pred = my_predict(\n",
    "        mod2,\n",
    "        {   'raw_features': xtrain_roll[:n_show],\n",
    "            'raw_targets':  ytrain_roll[:n_show],\n",
    "        },\n",
    "        ztrain_roll.index[:n_show],\n",
    "    )\n",
    "    # ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mod2.save('data/processed/1.0-model-epoch_xxx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# forward and backward in stretching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# till 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on non-balanced series\n",
    "n_show = 1000*5\n",
    "ytrain_pred = my_predict(\n",
    "    mod2,\n",
    "    {   'raw_features': xtrain_roll[:n_show],\n",
    "        'raw_targets':  ytrain_roll[:n_show] + 1.5,\n",
    "    },\n",
    "    ztrain_roll.index[:n_show],\n",
    ")\n",
    "ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# stop at 290"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# till n_stretch = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(\n",
    "    ytrain_roll[:,-1,0].astype('int16'),\n",
    "    index = ztrain_roll.index\n",
    ").plot(label='actual', figsize=(20,3), style='-', alpha=0.5)\n",
    "\n",
    "ytrain_pred['prediction'].plot(label='predicted', style='-', figsize=(20,3), alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "# plt.title(city)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = train_features.shape[0] - n_train\n",
    "x_test = train_features.tail(n=n_test).copy()\n",
    "y_test = train_targets.tail(n=n_test).copy()\n",
    "print('x_test, y_test', x_test.shape, y_test.shape)\n",
    "\n",
    "xtest_roll, ytest_roll, ztest_roll = preprocess(x_test, y_test)\n",
    "xtest_roll.shape, ytest_roll.shape, ztest_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = 1000*5\n",
    "ytest_pred = my_predict(\n",
    "    mod2,\n",
    "    {   'raw_features': xtest_roll[:n_show],\n",
    "        'raw_targets':  ytest_roll[:n_show] + 1.1,\n",
    "    },\n",
    "    ztest_roll.index[:n_show],\n",
    ")\n",
    "ytest_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on new subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj2_features, subj2_targets = my_load(subj_ids = [2], series_ids = [x+1 for x in range(8)])\n",
    "subj2_features.shape, subj2_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subj2 = subj2_features.copy()\n",
    "y_subj2 = subj2_targets.copy()\n",
    "print('x_subj2, y_subj2', x_subj2.shape, y_subj2.shape)\n",
    "\n",
    "xsubj2_roll, ysubj2_roll, zsubj2_roll = preprocess(x_subj2, y_subj2)\n",
    "assert xsubj2_roll.shape[0] > 0\n",
    "xsubj2_roll.shape, ysubj2_roll.shape, zsubj2_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = 1000*5\n",
    "ysubj2_pred = my_predict(\n",
    "    mod2,\n",
    "    {   'raw_features': xsubj2_roll[:n_show],\n",
    "        'raw_targets':  ysubj2_roll[:n_show] + 1.1,\n",
    "    },\n",
    "    zsubj2_roll.index[:n_show],\n",
    ")\n",
    "ysubj2_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

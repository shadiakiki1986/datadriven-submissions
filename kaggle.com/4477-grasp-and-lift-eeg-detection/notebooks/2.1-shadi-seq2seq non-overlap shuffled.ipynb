{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes from 1.0\n",
    "\n",
    "- simpler architecture: lstm of feature sequences directly to target sequences\n",
    "  - similar to [Creating A Text Generator Using Recurrent Neural Network ](https://chunml.github.io/ChunML.github.io/project/Creating-Text-Generator-Using-Recurrent-Neural-Network/)\n",
    "- min/max scaling be per series\n",
    "- model's \"subtract\" output MAE with target = zeros changed to \"binary cross-entropy\"\n",
    "\n",
    "- check if better to just use multi-core with smaller batch-size ~ 32 than GPU\n",
    "    - indeed it is\n",
    "    - batch size 16*1024 .. Epoch 33/400    - 7s - loss: 0.6803 - val_loss: 0.6792\n",
    "    - batch size     512\n",
    "      - Epoch 1/400    - 22s - loss: 0.6700 - val_loss: 0.6361\n",
    "      - Epoch 2/400    - 21s - loss: 0.6558 - val_loss: 0.6312\n",
    "    - note that `batch_size=32` in benchmarks [here](https://www.tensorflow.org/performance/performance_guide#optimizing_for_cpu)\n",
    "\n",
    "\n",
    "- is downsampling harmful for training/prediction?\n",
    "  - downsample_pts = 1 is very very slow\n",
    "- Add AveragePooling1D layer as base (10x downsampling)\n",
    "  - validation loss was horrible\n",
    "- ~~instead of striding, use non-overlapping windows~~\n",
    "  - turns out that eventhough the training was good, the predictions were not\n",
    "  - maybe because lstm requires that input not be shuffled\n",
    "  - another point was how I increment the dataset to have balanced classes of HandStart=0 and HandStart=1\n",
    "  - TODO train the LSTM with balanced classes, no overlap, and without shuffling\n",
    "  \n",
    "- TODO adding Conv1D ahead of LSTM\n",
    "- TODO also interesting is [ConvLSTM2D](https://keras.io/layers/recurrent/#convlstm2d)\n",
    "  - and the [keras example](https://github.com/keras-team/keras/blob/ce4947cbaf380589a63def4cc6eb3e460c41254f/examples/conv_lstm.py)\n",
    "  \n",
    "- TODO lahead is currently in \"points\". So needs to be changed depending on downsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check gpu usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~set multi-core~~\n",
    "\n",
    "- [ref 0](https://stackoverflow.com/a/49530131/4126114)\n",
    "- [ref 1](https://datascience.stackexchange.com/a/22840/35596)\n",
    "- [ref 2](https://stackoverflow.com/a/45843766/4126114)\n",
    "\n",
    "Didn't help ... on c5.18xlarge still using 10 cores maximum, even if 72 are available"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend import tensorflow_backend as K"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_thread = 72\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_thread,\n",
    "                        allow_soft_placement=True)\n",
    "                        # allow_soft_placement=False)\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_train: number of points for training, as opposed to testing\n",
    "# lahead: stride data with \"lahead\" window size\n",
    "# batch_size: keras.model.fit parameter .. smaller batches lead to less loss of data when truncating non-multiples of batch_size\n",
    "# downsample_pts: 1 for no downsampling, 10 for downsample by 10\n",
    "#---------------------------------------------------------\n",
    "# set 1\n",
    "# n_train, lahead, batch_size, downsample_pts = 120000, 10, 2**14, 10\n",
    "\n",
    "# set 2\n",
    "# n_train, lahead, batch_size, downsample_pts = 1200000, 100, (2**10)*(2**8), 1 # batch_size = 1024\n",
    "# n_train, lahead, batch_size, downsample_pts = 1200000, 100, 2**8, 1 # batch_size = 256\n",
    "\n",
    "# note that lahead=150 matches perfectly with non-downsampled length of HandStart=1 length\n",
    "# note smaller batch_size since non-overlap causes smaller number of HanStart=1 samples\n",
    "n_train, lahead, batch_size, downsample_pts = 1200000, 150, 2**5, 1 # batch_size = 32\n",
    "\n",
    "# set 3:\n",
    "# training each subject / series separately\n",
    "# Requires smaller batch_size since each series is only around 1000 pts when downsampled by 10\n",
    "# n_train, lahead, batch_size, downsample_pts = 120000, 10, 2**4, 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Dense, LSTM, Lambda, Dropout, Embedding, Flatten,\n",
    "                         Subtract, Dot, Activation,\n",
    "                         Input, RepeatVector, TimeDistributed, Concatenate,\n",
    "                         Conv1D, MaxPooling1D, AveragePooling1D\n",
    "                         )\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load(subj_ids:list, series_ids:list):\n",
    "    features_all = []\n",
    "    targets_all = []\n",
    "    for i1 in subj_ids:\n",
    "        for i2 in series_ids:\n",
    "            for i3, fn in [\n",
    "                ('features', 'data/raw/train/subj%i_series%i_data.csv'%(i1, i2)),\n",
    "                ('targets', 'data/raw/train/subj%i_series%i_events.csv'%(i1, i2)),\n",
    "            ]:\n",
    "                print('status', i1, i2, i3)\n",
    "                xxx_i = pd.read_csv(fn)\n",
    "                xxx_i['subj_id'] = i1\n",
    "                xxx_i['series_id'] = i2\n",
    "                xxx_i = xxx_i.set_index(['subj_id', 'series_id', 'id']).astype('int16')\n",
    "                xxx_i = xxx_i[::downsample_pts] # downsample\n",
    "                if i3=='features':\n",
    "                    features_all.append(xxx_i)\n",
    "                else:\n",
    "                    targets_all.append(xxx_i)\n",
    "            \n",
    "    features_all = pd.concat(features_all, axis=0)\n",
    "    targets_all = pd.concat(targets_all, axis=0)\n",
    "    return features_all, targets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets = my_load(subj_ids = [1], series_ids = [x+1 for x in range(8)])\n",
    "train_features.shape, train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess features\n",
    "\n",
    "e.g. scale to [0,1], stride, truncate, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_df(df, n_back):\n",
    "    \"\"\"\n",
    "    create rolling windows for LSTM\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(n_back):\n",
    "        out.append(df.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[(n_back-1):, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    out = np.flip(out, axis=1) # so that the index=0 is the oldest, and index=4 is latest\n",
    "    return out\n",
    "\n",
    "\n",
    "def nonstride_df(a:np.array, n_back):\n",
    "    \"\"\"\n",
    "    create non-rolling windows (numpy)\n",
    "    \"\"\"\n",
    "    # truncate non-multiples of n_back\n",
    "    to_drop = a.shape[0] % n_back\n",
    "    print('nonstride_df, a.shape[0], to_drop', a.shape[0], to_drop)\n",
    "    a = a[to_drop:]\n",
    "    return a.reshape((-1,n_back,a.shape[1]))\n",
    "\n",
    "\n",
    "stride_df_20 = lambda df: stride_df   (df       , lahead)\n",
    "stride_df_21 = lambda df: nonstride_df(df.values, lahead)\n",
    "stride_df_22 = lambda  a: nonstride_df(a,         lahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_truncate(df):\n",
    "    \"\"\"\n",
    "    drop 1st x rows if they are not a multiple of batch_size\n",
    "    \"\"\"\n",
    "    return df.tail(df.shape[0] - (df.shape[0]%batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_pd_df(xxx, func):\n",
    "    return pd.DataFrame(\n",
    "             func(xxx), \n",
    "             columns=xxx.columns, \n",
    "             index=xxx.index\n",
    "           )\n",
    "\n",
    "def preprocess(x_train, y_train, with_overlap):\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    print('min/max start')\n",
    "    # xtrain_pre = x_train.groupby(['subj_id', 'series_id']).apply(lambda xxx: scaler.fit_transform(xxx))\n",
    "    xtrain_pre = ( x_train.groupby(['subj_id', 'series_id'])\n",
    "                          .apply(lambda xxx: wrap_pd_df(xxx, lambda yyy: scaler.fit_transform(yyy)))\n",
    "                 )\n",
    "    ytrain_pre = y_train # just a copy since no scaling done\n",
    "\n",
    "    print('train_pre', xtrain_pre.shape, ytrain_pre.shape)\n",
    "    #--------------------------------------\n",
    "    # FIXME striding can be parallelized\n",
    "    \n",
    "    stride_func = stride_df_20 if with_overlap else stride_df_21\n",
    "        \n",
    "    \n",
    "    # xtrain_roll = stride_df_2(xtrain_pre)\n",
    "    # ytrain_roll = stride_df_2(ytrain_pre)\n",
    "    xtrain_roll = (xtrain_pre.groupby(['subj_id', 'series_id'])\n",
    "                             .apply(stride_func)\n",
    "                             # .apply(lambda xxx: wrap_pd_df(xxx, stride_df_2))\n",
    "                  )\n",
    "    ytrain_roll = (ytrain_pre.groupby(['subj_id', 'series_id'])\n",
    "                             .apply(stride_func)\n",
    "                             # .apply(lambda xxx: wrap_pd_df(xxx, stride_df_2))\n",
    "                  )\n",
    "\n",
    "    # \"meta\" dataframe that will still contain the pandas index (above *_roll variables are numpy matrices)\n",
    "    # ztrain_roll = y_train.groupby(['subj_id', 'series_id']).apply(lambda group: group.iloc[(lahead-1):])\n",
    "    if with_overlap:\n",
    "        ztrain_roll = y_train.groupby(by=['subj_id', 'series_id']).apply(lambda group: group.iloc[(lahead-1):].reset_index(drop=True))\n",
    "    else:\n",
    "        ztrain_roll = stride_df_22(ytrain_pre.iloc[:,0].reset_index().index.values.reshape((-1,1)))\n",
    "        ztrain_roll = y_train.iloc[ztrain_roll[:,-1,0]]\n",
    "\n",
    "\n",
    "    print('train_roll 1', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    #return xtrain_roll, ytrain_roll, ztrain_roll\n",
    "\n",
    "    \"\"\"\n",
    "    # drop non-batchsize-multiple per subject/series pair\n",
    "    for (subj_id, series_id), group in xtrain_roll.groupby(['subj_id', 'series_id']):\n",
    "        to_drop = group.values[0].shape[0] % batch_size\n",
    "        print(subj_id, series_id, 'drop non-multiple', to_drop)\n",
    "        assert to_drop < 1000\n",
    "\n",
    "        xtrain_roll.loc[subj_id, series_id] = xtrain_roll.loc[subj_id, series_id][(to_drop):]\n",
    "        ytrain_roll.loc[subj_id, series_id] = ytrain_roll.loc[subj_id, series_id][(to_drop):]\n",
    "       \n",
    "    ztrain_roll = ztrain_roll.groupby(['subj_id', 'series_id']).apply(my_truncate)\n",
    "    print('train_roll 2', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # aggregate all strided matrices (since done per subj_id and series_id)\n",
    "    xtrain_roll = np.concatenate(xtrain_roll.values, axis=0)\n",
    "    ytrain_roll = np.concatenate(ytrain_roll.values, axis=0)\n",
    "    \n",
    "    # drop non-batchsize-multiple, once for all\n",
    "    to_drop = xtrain_roll.shape[0] % batch_size\n",
    "    print('drop non-multiple of batch_size', to_drop)\n",
    "    xtrain_roll = xtrain_roll[(to_drop):]\n",
    "    ytrain_roll = ytrain_roll[(to_drop):]\n",
    "    ztrain_roll = my_truncate(ztrain_roll)\n",
    "    print('train_roll 2', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    \n",
    "    assert xtrain_roll.shape[0]>0, \"lost all data ... batch_size=%s is too high\"%batch_size\n",
    "    \n",
    "    return xtrain_roll, ytrain_roll, ztrain_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features.head(n=n_train).copy()\n",
    "y_train = train_targets.head(n=n_train).copy()\n",
    "print('x_train, y_train', x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ratio of HandStart = 0 to = 1 to get the target class imbalance\n",
    "ratio_0_1 = y_train.groupby('HandStart').size()\n",
    "print(ratio_0_1)\n",
    "ratio_0_1 = ratio_0_1.loc[0] // ratio_0_1.loc[1]\n",
    "ratio_0_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify each HandStart=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate length of HandStart == 1\n",
    "y_cols = y_train.columns\n",
    "for k in y_cols: # e.g. 'HandStart'\n",
    "    y_temp1 = y_train[k].diff().fillna(value=0)\n",
    "    y_temp2 = y_temp1.copy()\n",
    "    y_temp2[y_temp2 < 0] = 0\n",
    "    y_temp2 = y_temp2.cumsum()\n",
    "    y_train['%s_id'%k] = y_train[k] * y_temp2\n",
    "\n",
    "y_train[[x for x in y_train.columns if x.endswith('_id')]].head(n=10000).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all HandStarts are of length 150\n",
    "assert set(y_train[y_train['HandStart_id']>0].groupby('HandStart_id').size()) == set([150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train[y_train['LiftOff_id']>0].groupby('LiftOff_id').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create separate non-overlapping windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create separate non-overlapping windows from HandStart=1 and =0, and then concatenate\n",
    "# This way, we don't get a window half of which has HandStart=0 and the other half = 1\n",
    "xtrain_roll_1, ytrain_roll_1, ztrain_roll_1 = preprocess(x_train[y_train['HandStart']==1], y_train[y_train['HandStart']==1], with_overlap=False)\n",
    "xtrain_roll_0, ytrain_roll_0, ztrain_roll_0 = preprocess(x_train[y_train['HandStart']==0], y_train[y_train['HandStart']==0], with_overlap=False)\n",
    "\n",
    "# repeat the *_1 40x times to balance against the *_0 class (check above for how to calculate 40)\n",
    "xtrain_roll_1 = np.repeat(xtrain_roll_1, repeats=ratio_0_1, axis=0)\n",
    "ytrain_roll_1 = np.repeat(ytrain_roll_1, repeats=ratio_0_1, axis=0)\n",
    "\n",
    "z_np = np.repeat(ztrain_roll_1.values, repeats=ratio_0_1, axis=0)\n",
    "z_cols = ztrain_roll_1.columns\n",
    "z_inds = ztrain_roll_1.index\n",
    "print('z shape', ztrain_roll_1.shape, z_np.shape, z_cols.shape, z_inds.shape)\n",
    "ztrain_roll_1 = pd.DataFrame(\n",
    "    z_np,\n",
    "    columns = z_cols,\n",
    "    index = z_inds.repeat(ratio_0_1)\n",
    ")\n",
    "\n",
    "# concatenate _0 with _1\n",
    "xtrain_roll = np.concatenate([xtrain_roll_1, xtrain_roll_0])\n",
    "ytrain_roll = np.concatenate([ytrain_roll_1, ytrain_roll_0])\n",
    "ztrain_roll = pd.concat([ztrain_roll_1, ztrain_roll_0])\n",
    "\n",
    "assert xtrain_roll.shape[0] > 0\n",
    "xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "197%32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[['HandStart']].head(n=10000).plot(figsize=(20,3), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting \":,-1,:\" below is like plotting every \"lahead\" points\n",
    "xtrain_plot = pd.DataFrame(xtrain_roll[:,-1,:], columns=x_train.columns)\n",
    "print(xtrain_plot.shape)\n",
    "xtrain_plot[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot below should have a balanced number of points for 1 and 0\n",
    "# if the 1's are repeated enough\n",
    "#\n",
    "# Plotting \":,-1,:\" below is like plotting every \"lahead\" points\n",
    "\n",
    "ytrain_plot = pd.DataFrame(ytrain_roll[:,-1,:], columns=y_train.columns)\n",
    "print(ytrain_plot.shape)\n",
    "ytrain_plot['HandStart'].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below should show {0,100} showing that all axis=1 dim is flat\n",
    "set(ytrain_roll[:,:,0].sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ind = np.arange(xtrain_roll.shape[0])\n",
    "np.random.shuffle(new_ind)\n",
    "xtrain_roll = xtrain_roll[new_ind]\n",
    "ytrain_roll = ytrain_roll[new_ind]\n",
    "ztrain_roll = ztrain_roll.iloc[new_ind]\n",
    "xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model: AE coupled with regression on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coupled():\n",
    "    lstm_dim_1 = 15\n",
    "    len_feat = xtrain_roll.shape[2]\n",
    "    len_targ = 1\n",
    "    input_shape = (lahead, len_feat, )\n",
    "\n",
    "    # input\n",
    "    feat_raw = Input(shape=input_shape, name='raw_features')\n",
    "    \n",
    "    # downsample\n",
    "    feat_conv = feat_raw\n",
    "    # feat_conv = AveragePooling1D(pool_size = 10)(feat_conv)\n",
    "\n",
    "    # convolve and maxpool\n",
    "    # feat_conv = Conv1D(filters = 32, kernel_size = 5, padding='valid', activation='relu', strides=1)(feat_conv)\n",
    "    # feat_conv = MaxPooling1D(pool_size = 4)(feat_conv)\n",
    "\n",
    "    # features encoder\n",
    "    feat_enc = feat_conv\n",
    "    feat_enc = LSTM(\n",
    "              lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=False,\n",
    "              activation='tanh',\n",
    "              name='encoded_features')(feat_enc)\n",
    "\n",
    "    # features decoder\n",
    "    targ_rec = feat_enc\n",
    "    targ_rec = RepeatVector(lahead, input_shape=(lstm_dim_1, ))(targ_rec)\n",
    "    targ_rec = LSTM(lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=True,\n",
    "              dropout=0.2,\n",
    "              activation='tanh')(targ_rec)\n",
    "    targ_rec = TimeDistributed(\n",
    "        # Dense(len_targ, activation='linear'),\n",
    "        Dense(len_targ, activation='sigmoid'),\n",
    "        name='reconstructed_targets'\n",
    "    )(targ_rec)\n",
    "\n",
    "    # create model\n",
    "    model_all = Model(inputs = [feat_raw], outputs = [targ_rec])\n",
    "    return model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.losses import binary_crossentropy\n",
    "def double_binary_crossentropy(y_true, y_pred):\n",
    "    return K.mean(binary_crossentropy(y_true, y_pred), axis=-1)\n",
    "\n",
    "\n",
    "mod2 = create_coupled()\n",
    "# mod2.compile(loss='mae', optimizer='adam')\n",
    "mod2.compile(loss=double_binary_crossentropy, optimizer='adam')\n",
    "# mod2.compile(loss=double_binary_crossentropy, optimizer='adam', sample_weight_mode=\"temporal\")\n",
    "mod2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(model, np_in, index):\n",
    "    \n",
    "    # make prediction\n",
    "    targ_rec = model.predict(np_in, batch_size=batch_size)\n",
    "        \n",
    "    # plot target reconstruction\n",
    "    feat_int = 0\n",
    "    pd.DataFrame({\n",
    "        'actual': pd.Series(np_in['raw_targets'][:,-1,feat_int],  index=index).astype('int16'),\n",
    "        'pred': pd.Series(targ_rec[:,-1,feat_int],  index=index),\n",
    "    }).plot(figsize=(20,3), alpha=0.5)\n",
    "    plt.title('target %i'%(feat_int))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # prepare output\n",
    "    out = pd.DataFrame({\n",
    "        'prediction': targ_rec[:,-1,0].squeeze(), \n",
    "        'id': index,\n",
    "    }).set_index(['id'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# train directly on full data, with assigning class_weight in model.fit\n",
    "print(time.ctime(),'fit start')\n",
    "history = mod2.fit(\n",
    "         {   'raw_features': xtrain_roll,\n",
    "         },\n",
    "         {   'reconstructed_targets': ytrain_roll[:,:,:1],\n",
    "         },\n",
    "         batch_size=batch_size,\n",
    "         epochs=150,\n",
    "         # initial_epoch = 17,\n",
    "         verbose=2,\n",
    "         #validation_data=None,\n",
    "         # class_weight = {0: 0.1, 1: 0.9}, # class_weight not supported for 3+ dimensional targets\n",
    "         sample_weight = (ytrain_roll[:,:,0]==1)*0.8 + 0.1,\n",
    "         shuffle=False\n",
    "    )\n",
    "print(time.ctime(),'fit end')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# construct wider masks of the HandStart = 1 and append in single matrix for training\n",
    "mask_init = (ytrain_roll[:,-1,0]==1)\n",
    "xtrain_bal = []\n",
    "ytrain_bal = []\n",
    "ztrain_bal = []\n",
    "for n_stretch in [20, 40, 60, 80, 100, 150, 200, 250, 300]:\n",
    "    print('status', n_stretch)\n",
    "    mask_stretched = stretch_mask(mask_init, n_stretch)\n",
    "\n",
    "    xtrain_bal.append(xtrain_roll[mask_stretched])\n",
    "    ytrain_bal.append(ytrain_roll[mask_stretched])\n",
    "    ztrain_bal.append(ztrain_roll[mask_stretched])\n",
    "    \n",
    "xtrain_bal = np.concatenate(xtrain_bal, axis=0)\n",
    "ytrain_bal = np.concatenate(ytrain_bal, axis=0)\n",
    "ztrain_bal = np.concatenate(ztrain_bal, axis=0)\n",
    "\n",
    "print('shape', xtrain_bal.shape, ytrain_bal.shape, ztrain_bal.shape)\n",
    "\n",
    "print(time.ctime(),'fit start')\n",
    "history = mod2.fit(\n",
    "         {   'raw_features': xtrain_bal,\n",
    "         },\n",
    "         {   'reconstructed_targets': ytrain_bal[:,:,:1],\n",
    "         },\n",
    "         batch_size=batch_size,\n",
    "         epochs=150,\n",
    "         # initial_epoch = 17,\n",
    "         verbose=2,\n",
    "         #validation_data=None,\n",
    "         shuffle=False\n",
    "    )\n",
    "print(time.ctime(),'fit end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(time.ctime(),'fit start')\n",
    "history = mod2.fit(\n",
    "         {   'raw_features': xtrain_roll,\n",
    "         },\n",
    "         {   'reconstructed_targets': ytrain_roll[:,:,:1],\n",
    "         },\n",
    "         batch_size=batch_size,\n",
    "         epochs=100,\n",
    "         # initial_epoch = 17,\n",
    "         verbose=2,\n",
    "         #validation_data=None,\n",
    "         validation_split = 0.3,\n",
    "         shuffle=False\n",
    "    )\n",
    "print(time.ctime(),'fit end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore first few points since large relative to others\n",
    "# plt.plot(history.history['loss'][5:], label='loss')\n",
    "plt.plot(history.history['loss'], label='loss') # [5:]\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on whole series (plots implicitly actual vs predicted)\n",
    "n_show = 100\n",
    "for n_start in [0,  50, 100, 150]:\n",
    "    ytrain_pred = my_predict(\n",
    "        mod2,\n",
    "        {   'raw_features': xtrain_roll[n_start:(n_start+n_show)],\n",
    "            'raw_targets':  ytrain_roll[n_start:(n_start+n_show)] + 1,\n",
    "        },\n",
    "        ztrain_roll.index[:n_show],\n",
    "    )\n",
    "    # ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test if model can predict on shuffled data\n",
    "xtrain_shuffled = xtrain_roll.copy()\n",
    "ytrain_shuffled = ytrain_roll.copy()\n",
    "np.random.shuffle(xtrain_shuffled)\n",
    "np.random.shuffle(ytrain_shuffled)\n",
    "z_ind = ztrain_roll.reset_index().index.values\n",
    "np.random.shuffle(z_ind)\n",
    "ztrain_shuffled = ztrain_roll.iloc[z_ind]\n",
    "\n",
    "n_show = 100\n",
    "for n_start in [0,  50, 100, 150]:\n",
    "    ytrain_pred = my_predict(\n",
    "        mod2,\n",
    "        {   'raw_features': xtrain_shuffled[n_start:(n_start+n_show)],\n",
    "            'raw_targets':  ytrain_shuffled[n_start:(n_start+n_show)] + 1,\n",
    "        },\n",
    "        ztrain_shuffled.index[:n_show],\n",
    "    )\n",
    "    # ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mod2.save('data/processed/1.0-model-epoch_xxx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-build data without balancing and shuffling and with overlap\n",
    "n_show = 100000\n",
    "xtrain_ori, ytrain_ori, ztrain_ori = preprocess(x_train.head(n=n_show), y_train.head(n=n_show), with_overlap=True)\n",
    "print(xtrain_ori.shape, ytrain_ori.shape, ztrain_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_plot = pd.DataFrame(xtrain_ori[:,-1,:], columns=x_train.columns)\n",
    "print(xtrain_plot.shape)\n",
    "xtrain_plot[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_plot = pd.DataFrame(ytrain_ori[:,-1,:], columns=y_train.columns)\n",
    "print(ytrain_plot.shape)\n",
    "ytrain_plot['HandStart'].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ytrain_pred = my_predict(\n",
    "    mod2,\n",
    "    {   'raw_features': xtrain_ori,\n",
    "        'raw_targets':  ytrain_ori + 1.1,\n",
    "    },\n",
    "    ztrain_ori.index,\n",
    ")\n",
    "# ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(\n",
    "    ytrain_roll[:,-1,0].astype('int16'),\n",
    "    index = ztrain_roll.index\n",
    ").plot(label='actual', figsize=(20,3), style='-', alpha=0.5)\n",
    "\n",
    "ytrain_pred['prediction'].plot(label='predicted', style='-', figsize=(20,3), alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "# plt.title(city)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = train_features.shape[0] - n_train\n",
    "x_test = train_features.tail(n=n_test).copy()\n",
    "y_test = train_targets.tail(n=n_test).copy()\n",
    "print('x_test, y_test', x_test.shape, y_test.shape)\n",
    "\n",
    "xtest_roll, ytest_roll, ztest_roll = preprocess(x_test, y_test)\n",
    "xtest_roll.shape, ytest_roll.shape, ztest_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = 10000*5\n",
    "ytest_pred = my_predict(\n",
    "    mod2,\n",
    "    {   'raw_features': xtest_roll[:n_show],\n",
    "        'raw_targets':  ytest_roll[:n_show] + 1.1,\n",
    "    },\n",
    "    ztest_roll.index[:n_show],\n",
    ")\n",
    "ytest_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on new subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj2_features, subj2_targets = my_load(subj_ids = [2], series_ids = [x+1 for x in range(8)])\n",
    "subj2_features.shape, subj2_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subj2 = subj2_features.copy()\n",
    "y_subj2 = subj2_targets.copy()\n",
    "print('x_subj2, y_subj2', x_subj2.shape, y_subj2.shape)\n",
    "\n",
    "xsubj2_roll, ysubj2_roll, zsubj2_roll = preprocess(x_subj2, y_subj2)\n",
    "assert xsubj2_roll.shape[0] > 0\n",
    "xsubj2_roll.shape, ysubj2_roll.shape, zsubj2_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_show = 1000*5\n",
    "ysubj2_pred = my_predict(\n",
    "    mod2,\n",
    "    {   'raw_features': xsubj2_roll[:n_show],\n",
    "        'raw_targets':  ysubj2_roll[:n_show] + 1.1,\n",
    "    },\n",
    "    zsubj2_roll.index[:n_show],\n",
    ")\n",
    "ysubj2_pred.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

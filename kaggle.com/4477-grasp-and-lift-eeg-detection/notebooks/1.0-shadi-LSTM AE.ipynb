{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take at EEG - hand motion data\n",
    "\n",
    "Will try to fit to first subject's data only\n",
    "\n",
    "- training on series 1-6, testing on series 7-8\n",
    "- TODO train on downsampled every 10 points for faster computation\n",
    "  - harmful for training/prediction?\n",
    "- TODO should the min/max scaling be per series?\n",
    "- TODO lahead is currently in \"points\". So needs to be changed depending on downsampling.\n",
    "- TODO model \"subtract\" output is measured as squared error (with target = zeros)\n",
    "  - can I do better? something like \"binary cross-entropy\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 120000 # number of points for training, as opposed to testing\n",
    "\n",
    "lahead = 10 # 60 yields no classification results\n",
    "batch_size = 2**10 # 2**4 # 2**10 # smaller batches lead to less loss of data when truncating non-multiples of batch_size\n",
    "\n",
    "downsample_pts = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Dropout, Embedding, Flatten, Subtract, Dot\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, RepeatVector, TimeDistributed, Concatenate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load(subj_ids:list, series_ids:list):\n",
    "    features_all = []\n",
    "    targets_all = []\n",
    "    for i1 in subj_ids:\n",
    "        for i2 in series_ids:\n",
    "            for i3, fn in [\n",
    "                ('features', 'data/raw/train/subj%i_series%i_data.csv'%(i1, i2)),\n",
    "                ('targets', 'data/raw/train/subj%i_series%i_events.csv'%(i1, i2)),\n",
    "            ]:\n",
    "                xxx_i = pd.read_csv(fn)\n",
    "                xxx_i = xxx_i.set_index('id').astype('int16')\n",
    "                xxx_i = xxx_i[::downsample_pts] # downsample\n",
    "                if i3=='features':\n",
    "                    features_all.append(xxx_i)\n",
    "                else:\n",
    "                    targets_all.append(xxx_i)\n",
    "            \n",
    "    features_all = pd.concat(features_all, axis=0)\n",
    "    targets_all = pd.concat(targets_all, axis=0)\n",
    "    return features_all, targets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets = my_load(subj_ids = [1], series_ids = [x+1 for x in range(8)])\n",
    "train_features.shape, train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess features\n",
    "\n",
    "e.g. scale to [0,1], stride, truncate, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_df(df, n_back):\n",
    "    \"\"\"\n",
    "    create rolling windows for LSTM\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(n_back):\n",
    "        out.append(df.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[(n_back-1):, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    out = np.flip(out, axis=1) # so that the index=0 is the oldest, and index=4 is latest\n",
    "    return out\n",
    "\n",
    "stride_df_2 = lambda x: stride_df(x, lahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_truncate(df):\n",
    "    \"\"\"\n",
    "    drop 1st x rows if they are not a multiple of batch_size\n",
    "    \"\"\"\n",
    "    return df.tail(df.shape[0] - (df.shape[0]%batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x_train, y_train):\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    xtrain_pre = scaler.fit_transform(x_train)\n",
    "    xtrain_pre = pd.DataFrame(xtrain_pre, columns=x_train.columns, index=x_train.index)\n",
    "    ytrain_pre = y_train\n",
    "\n",
    "    print('train_pre', xtrain_pre.shape, ytrain_pre.shape)\n",
    "    #--------------------------------------\n",
    "    xtrain_roll = stride_df_2(xtrain_pre)\n",
    "    ytrain_roll = stride_df_2(ytrain_pre)\n",
    "\n",
    "    # \"meta\" dataframe that will still contain the pandas index (above *_roll variables are numpy matrices)\n",
    "    ztrain_roll = y_train.apply(lambda group: group.iloc[(lahead-1):])\n",
    "\n",
    "    print('train_roll 1', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    #--------------------------------------\n",
    "    # drop non-batchsize-multiple per city\n",
    "    to_drop = xtrain_roll.shape[0] % batch_size\n",
    "    print('drop non-multiple', to_drop)\n",
    "\n",
    "    xtrain_roll = xtrain_roll[(to_drop):]\n",
    "    ytrain_roll = ytrain_roll[(to_drop):]\n",
    "\n",
    "    ztrain_roll = my_truncate(ztrain_roll)\n",
    "\n",
    "    print('train_roll 2', xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape)\n",
    "    \n",
    "    return xtrain_roll, ytrain_roll, ztrain_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features.head(n=n_train).copy()\n",
    "y_train = train_targets.head(n=n_train).copy()\n",
    "print('x_train, y_train', x_train.shape, y_train.shape)\n",
    "\n",
    "xtrain_roll, ytrain_roll, ztrain_roll = preprocess(x_train, y_train)\n",
    "assert xtrain_roll.shape[0] > 0\n",
    "xtrain_roll.shape, ytrain_roll.shape, ztrain_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[['Fp1', 'Fp2']].plot(figsize=(20,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model: AE coupled with regression on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coupled():\n",
    "    lstm_dim_1 = 15\n",
    "    len_feat = xtrain_roll.shape[2]\n",
    "    len_targ = 1\n",
    "    input_shape = (lahead, len_feat, )\n",
    "\n",
    "    # features encoder\n",
    "    feat_raw = Input(shape=input_shape, name='raw_features')\n",
    "    feat_enc = feat_raw\n",
    "    feat_enc = LSTM(\n",
    "              lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=False,\n",
    "              activation='tanh',\n",
    "              name='encoded_features')(feat_enc)\n",
    "\n",
    "    # features decoder\n",
    "    feat_rec = feat_enc\n",
    "    feat_rec = RepeatVector(lahead, input_shape=(lstm_dim_1, ))(feat_rec)\n",
    "    feat_rec = LSTM(lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=True,\n",
    "              dropout=0.2,\n",
    "              activation='tanh')(feat_rec)\n",
    "    feat_rec = TimeDistributed(\n",
    "        Dense(len_feat, activation='linear'),\n",
    "        name='reconstructed_features'\n",
    "    )(feat_rec)\n",
    "\n",
    "    # target encoder\n",
    "    targ_raw = Input(shape=(lahead, len_targ, ), name='raw_targets')\n",
    "    targ_enc = targ_raw\n",
    "    targ_enc = LSTM(\n",
    "              lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=False,\n",
    "              activation='tanh',\n",
    "              name='encoded_targets')(targ_enc)\n",
    "\n",
    "    # target decoder\n",
    "    targ_rec = targ_enc\n",
    "    targ_rec = RepeatVector(lahead, input_shape=(lstm_dim_1, ), name='targ_dec_1')(targ_rec)\n",
    "    targ_rec = LSTM(lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=True,\n",
    "              dropout=0.2,\n",
    "              activation='tanh', name='targ_dec_2')(targ_rec)\n",
    "    targ_rec = TimeDistributed(\n",
    "        Dense(len_targ, activation='linear'),\n",
    "        name='reconstructed_targets'\n",
    "    )(targ_rec)\n",
    "\n",
    "    # internal regressor\n",
    "    out = feat_enc\n",
    "    out = Dense(100           , activation='relu', name='pre_targ_dec_1')(out)\n",
    "    out = Dense(lstm_dim_1    , activation='linear', name='pre_targ_dec_2')(out) # match shape of targ_enc\n",
    "    \n",
    "    # subtract the prediction of the encoded features from the encoded target\n",
    "    to_be_zero = Subtract()([out, targ_enc])\n",
    "    to_be_zero = Dot(axes=-1, name='regressed_output')([to_be_zero, to_be_zero])\n",
    "\n",
    "    # create model\n",
    "    # model_all = Model(inputs = [feat_raw, is_epidemic, weekofyear], outputs = [feat_rec, out])\n",
    "    model_all = Model(inputs = [feat_raw, targ_raw], outputs = [feat_rec, targ_rec, to_be_zero])\n",
    "    model_all.compile(loss='mae', optimizer='adam')\n",
    "    return model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod2 = create_coupled()\n",
    "mod2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual fit\n",
    "print(time.ctime(),'fit start')\n",
    "history = mod2.fit(\n",
    "         {   # ...[ytrain_roll['is_epidemic']], to only train on subset of epidemics\n",
    "             'raw_features': xtrain_roll,\n",
    "             'raw_targets': ytrain_roll[:,:,:1],\n",
    "         },\n",
    "         {   'reconstructed_features': xtrain_roll,\n",
    "             'reconstructed_targets': ytrain_roll[:,:,:1],\n",
    "             'regressed_output': ytrain_roll[:,-1,0]*0, # zeros\n",
    "         },\n",
    "         batch_size=batch_size,\n",
    "         epochs=3000,\n",
    "         initial_epoch = 17,\n",
    "         verbose=2,\n",
    "         #validation_data=None,\n",
    "         shuffle=False\n",
    "    )\n",
    "print(time.ctime(),'fit end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ignore first few points since large relative to others\n",
    "plt.plot(history.history['loss'], label='loss') # [5:]\n",
    "#plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "# plt.title(city)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mod2.save('data/processed/1.0-model-epoch_xxx.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract prediction model from features to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/blob/master/examples/mnist_transfer_cnn.py#L89\n",
    "\n",
    "i1 = Input(shape=(lahead, xtrain_roll.shape[2]), name='raw_features')\n",
    "m1 = mod2.get_layer(name='encoded_features')(i1)\n",
    "\n",
    "m3 = m1\n",
    "m3 = mod2.get_layer(name='pre_targ_dec_1')(m3)\n",
    "m3 = mod2.get_layer(name='pre_targ_dec_2')(m3)\n",
    "m3 = mod2.get_layer(name='targ_dec_1')(m3)\n",
    "m3 = mod2.get_layer(name='targ_dec_2')(m3)\n",
    "m3 = mod2.get_layer(name='reconstructed_targets')(m3)\n",
    "\n",
    "mod3 = Model(inputs = [i1], outputs = [m3])\n",
    "mod3.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "mod3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def my_predict(np_in, index):\n",
    "    \n",
    "    # make prediction\n",
    "    targ_rec = mod3.predict(np_in, batch_size=batch_size)\n",
    "        \n",
    "    # plot target reconstruction\n",
    "    feat_int = 0\n",
    "    pd.DataFrame({\n",
    "        'actual': pd.Series(np_in['raw_targets'][:,-1,feat_int],  index=index).astype('int16'),\n",
    "        'pred': pd.Series(targ_rec[:,-1,feat_int],  index=index),\n",
    "    }).plot(figsize=(20,3), alpha=0.5)\n",
    "    plt.title('target %i'%(feat_int))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # prepare output\n",
    "    out = pd.DataFrame({\n",
    "        'prediction': targ_rec[:,-1,0].squeeze(), \n",
    "        'id': index,\n",
    "    }).set_index(['id'])\n",
    "    return out\n",
    "\n",
    "#------------------------------------\n",
    "\n",
    "ytrain_pred = my_predict(\n",
    "    {   'raw_features': xtrain_roll,\n",
    "        'raw_targets':  ytrain_roll,\n",
    "    },\n",
    "    ztrain_roll.index,\n",
    ")\n",
    "ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.Series(\n",
    "    ytrain_roll[:,-1,0].astype('int16'),\n",
    "    index = ztrain_roll.index\n",
    ").plot(label='actual', figsize=(20,3), style='-', alpha=0.5)\n",
    "\n",
    "ytrain_pred['prediction'].plot(label='predicted', style='-', figsize=(20,3), alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "# plt.title(city)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = train_features.shape[0] - n_train\n",
    "x_test = train_features.tail(n=n_test).copy()\n",
    "y_test = train_targets.tail(n=n_test).copy()\n",
    "print('x_test, y_test', x_test.shape, y_test.shape)\n",
    "\n",
    "xtest_roll, ytest_roll, ztest_roll = preprocess(x_test, y_test)\n",
    "xtest_roll.shape, ytest_roll.shape, ztest_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest_pred = my_predict(\n",
    "    {   'raw_features': xtest_roll,\n",
    "        'raw_targets':  ytest_roll,\n",
    "    },\n",
    "    ztest_roll.index,\n",
    ")\n",
    "ytest_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

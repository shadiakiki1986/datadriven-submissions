{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes from 2.1\n",
    "\n",
    "- even simpler architecture: just regular classification, without use of sequences\n",
    "\n",
    "Results\n",
    "- training stagnates at a loss ~ 0.40\n",
    "- result is good for training, but does not generalize to test data nor other subject\n",
    "- TODO add dropout\n",
    "- TODO observed that volatility of HandStart=1 is much lower than HandStart=0 ... use that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check gpu usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_train: number of points for training, as opposed to testing\n",
    "# lahead: stride data with \"lahead\" window size\n",
    "# batch_size: keras.model.fit parameter .. smaller batches lead to less loss of data when truncating non-multiples of batch_size\n",
    "# downsample_pts: 1 for no downsampling, 10 for downsample by 10\n",
    "#---------------------------------------------------------\n",
    "# set 1\n",
    "# n_train, lahead, batch_size, downsample_pts = 120000, 10, 2**14, 10\n",
    "n_train, lahead, batch_size, downsample_pts = 120000, 10, 32, 10\n",
    "\n",
    "# set 2\n",
    "# n_train, lahead, batch_size, downsample_pts = 1200000, 100, (2**10)*(2**8), 1 # batch_size = 1024\n",
    "# n_train, lahead, batch_size, downsample_pts = 1200000, 100, 2**8, 1 # batch_size = 256\n",
    "\n",
    "# note that lahead=150 matches perfectly with non-downsampled length of HandStart=1 length\n",
    "# note smaller batch_size since non-overlap causes smaller number of HanStart=1 samples\n",
    "# n_train, lahead, batch_size, downsample_pts = 1200000, 150, 2**5, 1 # batch_size = 32\n",
    "\n",
    "# set 3:\n",
    "# training each subject / series separately\n",
    "# Requires smaller batch_size since each series is only around 1000 pts when downsampled by 10\n",
    "# n_train, lahead, batch_size, downsample_pts = 120000, 10, 2**4, 10\n",
    "\n",
    "# print\n",
    "n_train, lahead, batch_size, downsample_pts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import (Dense, LSTM, Lambda, Dropout, Embedding, Flatten,\n",
    "                         Subtract, Dot, Activation,\n",
    "                         Input, RepeatVector, TimeDistributed, Concatenate,\n",
    "                         Conv1D, MaxPooling1D, AveragePooling1D\n",
    "                         )\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_load(subj_ids:list, series_ids:list):\n",
    "    features_all = []\n",
    "    targets_all = []\n",
    "    for i1 in subj_ids:\n",
    "        for i2 in series_ids:\n",
    "            for i3, fn in [\n",
    "                ('features', 'data/raw/train/subj%i_series%i_data.csv'%(i1, i2)),\n",
    "                ('targets', 'data/raw/train/subj%i_series%i_events.csv'%(i1, i2)),\n",
    "            ]:\n",
    "                print('status', i1, i2, i3)\n",
    "                xxx_i = pd.read_csv(fn)\n",
    "                xxx_i['subj_id'] = i1\n",
    "                xxx_i['series_id'] = i2\n",
    "                xxx_i = xxx_i.set_index(['subj_id', 'series_id', 'id']).astype('int16')\n",
    "                xxx_i = xxx_i[::downsample_pts] # downsample\n",
    "                if i3=='features':\n",
    "                    features_all.append(xxx_i)\n",
    "                else:\n",
    "                    targets_all.append(xxx_i)\n",
    "            \n",
    "    features_all = pd.concat(features_all, axis=0)\n",
    "    targets_all = pd.concat(targets_all, axis=0)\n",
    "    return features_all, targets_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_targets = my_load(subj_ids = [1], series_ids = [x+1 for x in range(8)])\n",
    "train_features.shape, train_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split out training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_features.head(n=n_train).copy()\n",
    "y_train = train_targets.head(n=n_train).copy()\n",
    "print('x_train, y_train', x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate ratio of HandStart = 0 to = 1 to get the target class imbalance\n",
    "ratio_0_1 = y_train.groupby('HandStart').size()\n",
    "print(ratio_0_1)\n",
    "ratio_0_1 = ratio_0_1.loc[0] // ratio_0_1.loc[1]\n",
    "ratio_0_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify each HandStart=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate length of HandStart == 1\n",
    "y_cols = y_train.columns\n",
    "for k in y_cols: # e.g. 'HandStart'\n",
    "    y_temp1 = y_train[k].diff().fillna(value=0)\n",
    "    y_temp2 = y_temp1.copy()\n",
    "    y_temp2[y_temp2 < 0] = 0\n",
    "    y_temp2 = y_temp2.cumsum()\n",
    "    y_train['%s_id'%k] = y_train[k] * y_temp2\n",
    "\n",
    "y_train[[x for x in y_train.columns if x.endswith('_id')]].head(n=10000).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all HandStarts are of length 150\n",
    "assert set(y_train[y_train['HandStart_id']>0].groupby('HandStart_id').size()) == set([150 // downsample_pts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train[y_train['LiftOff_id']>0].groupby('LiftOff_id').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess features\n",
    "\n",
    "e.g. scale to [0,1], stride, truncate, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_truncate(df):\n",
    "    \"\"\"\n",
    "    drop 1st x rows if they are not a multiple of batch_size\n",
    "    \"\"\"\n",
    "    return df.tail(df.shape[0] - (df.shape[0]%batch_size))\n",
    "\n",
    "def wrap_pd_df(xxx, func):\n",
    "    return pd.DataFrame(\n",
    "             func(xxx), \n",
    "             columns=xxx.columns, \n",
    "             index=xxx.index\n",
    "           )\n",
    "\n",
    "def my_repeat(ztrain_roll_1):\n",
    "    z_np = np.repeat(ztrain_roll_1.values, repeats=ratio_0_1, axis=0)\n",
    "    z_cols = ztrain_roll_1.columns\n",
    "    z_inds = ztrain_roll_1.index\n",
    "    print('z shape', ztrain_roll_1.shape, z_np.shape, z_cols.shape, z_inds.shape)\n",
    "    ztrain_roll_1 = pd.DataFrame(\n",
    "        z_np,\n",
    "        columns = z_cols,\n",
    "        index = z_inds.repeat(ratio_0_1)\n",
    "    )\n",
    "    return ztrain_roll_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def preprocess(x_train, y_train, do_balance):\n",
    "    \n",
    "    #---------------------------\n",
    "    # scale\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    print('min/max start')\n",
    "    # xtrain_pre = x_train.groupby(['subj_id', 'series_id']).apply(lambda xxx: scaler.fit_transform(xxx))\n",
    "    xtrain_pre = ( x_train.groupby(['subj_id', 'series_id'])\n",
    "                          .apply(lambda xxx: wrap_pd_df(xxx, lambda yyy: scaler.fit_transform(yyy)))\n",
    "                 )\n",
    "    ytrain_pre = y_train # just a copy since no scaling done\n",
    "\n",
    "    print('train_pre', xtrain_pre.shape, ytrain_pre.shape)\n",
    "\n",
    "    #---------------------------\n",
    "    # balance classes\n",
    "    if do_balance:\n",
    "\n",
    "        # Create separate non-overlapping windows from HandStart=1 and =0, and then concatenate\n",
    "        # This way, we don't get a window half of which has HandStart=0 and the other half = 1\n",
    "        xtrain_roll_1, ytrain_roll_1 = (xtrain_pre[ytrain_pre['HandStart']==1], ytrain_pre[ytrain_pre['HandStart']==1])\n",
    "        xtrain_roll_0, ytrain_roll_0 = (xtrain_pre[ytrain_pre['HandStart']==0], ytrain_pre[ytrain_pre['HandStart']==0])\n",
    "\n",
    "        # repeat the *_1 40x times to balance against the *_0 class (check above for how to calculate 40)\n",
    "        xtrain_roll_1 = my_repeat(xtrain_roll_1)\n",
    "        ytrain_roll_1 = my_repeat(ytrain_roll_1)\n",
    "\n",
    "        # concatenate _0 with _1\n",
    "        xtrain_roll = pd.concat([xtrain_roll_1, xtrain_roll_0])\n",
    "        ytrain_roll = pd.concat([ytrain_roll_1, ytrain_roll_0])\n",
    "\n",
    "        assert xtrain_roll.shape[0] > 0\n",
    "        print('shape after balance', xtrain_roll.shape, ytrain_roll.shape)\n",
    "    else:\n",
    "        xtrain_roll = xtrain_pre\n",
    "        ytrain_roll = ytrain_pre\n",
    "\n",
    "    #---------------------------\n",
    "    # drop non-batchsize-multiple, once for all\n",
    "    to_drop = xtrain_roll.shape[0] % batch_size\n",
    "    print('drop non-multiple of batch_size', to_drop)\n",
    "    xtrain_roll = my_truncate(xtrain_roll)\n",
    "    ytrain_roll = my_truncate(ytrain_roll)\n",
    "    print('train_roll 2', xtrain_roll.shape, ytrain_roll.shape)\n",
    "\n",
    "    assert xtrain_roll.shape[0]>0, \"lost all data ... batch_size=%s is too high\"%batch_size\n",
    "    \n",
    "    return xtrain_roll, ytrain_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_roll, ytrain_roll = preprocess(x_train.copy(), y_train.copy(), True)\n",
    "xtrain_roll.shape, ytrain_roll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[['HandStart']].head(n=10000).plot(figsize=(20,3), alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_roll[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot below should have a balanced number of points for 1 and 0\n",
    "# if the 1's are repeated enough\n",
    "\n",
    "ytrain_roll['HandStart'].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ind = np.arange(xtrain_roll.shape[0])\n",
    "np.random.shuffle(new_ind)\n",
    "xtrain_roll = xtrain_roll.iloc[new_ind]\n",
    "ytrain_roll = ytrain_roll.iloc[new_ind]\n",
    "xtrain_roll.shape, ytrain_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_roll[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot below should have a balanced number of points for 1 and 0\n",
    "# if the 1's are repeated enough\n",
    "\n",
    "ytrain_roll['HandStart'].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model: simple classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coupled():\n",
    "    lstm_dim_1 = 15\n",
    "    len_feat = xtrain_roll.shape[-1]\n",
    "    len_targ = 1\n",
    "    input_shape = (len_feat, )\n",
    "\n",
    "    # input\n",
    "    feat_raw = Input(shape=input_shape, name='raw_features')\n",
    "    \n",
    "    # downsample\n",
    "    feat_conv = feat_raw\n",
    "    # feat_conv = AveragePooling1D(pool_size = 10)(feat_conv)\n",
    "\n",
    "    # features encoder\n",
    "    feat_enc = feat_conv\n",
    "    feat_enc = Dense(\n",
    "              lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              activation='relu',#'tanh',\n",
    "              name='intermediate')(feat_enc)\n",
    "\n",
    "    targ_rec = feat_enc\n",
    "    targ_rec = Dense(len_targ, activation='sigmoid', name='reconstructed_targets')(targ_rec)\n",
    "    targ_rec = Dropout(0.2)(targ_rec)\n",
    "    #targ_rec = Activation('sigmoid')(targ_rec)\n",
    "\n",
    "    # create model\n",
    "    model_all = Model(inputs = [feat_raw], outputs = [targ_rec])\n",
    "    return model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "\n",
    "mod2 = create_coupled()\n",
    "mod2.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "mod2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_predict(model, np_in, index):\n",
    "    \n",
    "    # make prediction\n",
    "    targ_rec = model.predict(np_in, batch_size=batch_size)\n",
    "        \n",
    "    # plot target reconstruction\n",
    "    feat_int = 0\n",
    "    pd.DataFrame({\n",
    "        'actual': pd.Series(np_in['raw_targets'][:,feat_int],  index=index).astype('int16'),\n",
    "        'pred': pd.Series(targ_rec[:,feat_int],                index=index),\n",
    "    }).plot(figsize=(20,3), alpha=0.5)\n",
    "    plt.title('target %i'%(feat_int))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # prepare output\n",
    "    out = pd.DataFrame({\n",
    "        'prediction': targ_rec[:,feat_int].squeeze(), \n",
    "        'id': index,\n",
    "    }).set_index(['id'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(time.ctime(),'fit start')\n",
    "history = mod2.fit(\n",
    "         {   'raw_features': xtrain_roll,\n",
    "         },\n",
    "         {   'reconstructed_targets': ytrain_roll.iloc[:,:1],\n",
    "         },\n",
    "         batch_size=batch_size,\n",
    "         epochs=100,\n",
    "         # initial_epoch = 17,\n",
    "         verbose=2,\n",
    "         #validation_data=None,\n",
    "         validation_split = 0.3,\n",
    "         shuffle=False\n",
    "    )\n",
    "print(time.ctime(),'fit end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore first few points since large relative to others\n",
    "# plt.plot(history.history['loss'][5:], label='loss')\n",
    "plt.plot(history.history['loss'], label='loss') # [5:]\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.title('training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# predict on whole series (plots implicitly actual vs predicted)\n",
    "#n_show = 100\n",
    "#n_start = 0 # ,  50, 100, 150]:\n",
    "#ytrain_pred = my_predict(\n",
    "#    mod2,\n",
    "#    {   'raw_features': xtrain_roll.iloc[n_start:(n_start+n_show)],\n",
    "#        'raw_targets':  ytrain_roll.iloc[n_start:(n_start+n_show)] + 1,\n",
    "#    },\n",
    "#    ytrain_roll.index[:n_show],\n",
    "#)\n",
    "ytrain_pred = my_predict(\n",
    "    mod2,\n",
    "    {   'raw_features': xtrain_roll.values[:100],\n",
    "        'raw_targets':  ytrain_roll.values[:100] + 1,\n",
    "    },\n",
    "    ytrain_roll.index[:100],\n",
    ")\n",
    "# ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-build data without balancing and shuffling and with overlap\n",
    "n_show = 100000\n",
    "xtrain_ori, ytrain_ori = preprocess(x_train.head(n=n_show), y_train.head(n=n_show), False)\n",
    "print(xtrain_ori.shape, ytrain_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_ori[['Fp1', 'Fp2']].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_ori['HandStart'].plot(figsize=(20,3), alpha=0.5)\n",
    "# plt.title('subj_id=1, series_id=1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    ytrain_pred = my_predict(\n",
    "        mod2,\n",
    "        {   'raw_features': xtrain_ori.values[i*1000:(i+1)*1000],\n",
    "            'raw_targets':  ytrain_ori.values[i*1000:(i+1)*1000] + 1,\n",
    "        },\n",
    "        ytrain_ori.index[i*1000:(i+1)*1000],\n",
    "    )\n",
    "    # ytrain_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = train_features.shape[0] - n_train\n",
    "x_test = train_features.tail(n=n_test).copy()\n",
    "y_test = train_targets.tail(n=n_test).copy()\n",
    "print('x_test, y_test', x_test.shape, y_test.shape)\n",
    "\n",
    "xtest_roll, ytest_roll = preprocess(x_test, y_test, False)\n",
    "xtest_roll.shape, ytest_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    ytest_pred = my_predict(\n",
    "        mod2,\n",
    "        {   'raw_features': xtest_roll.values[i*1000:(i+1)*1000],\n",
    "            'raw_targets':  ytest_roll.values[i*1000:(i+1)*1000] + 1,\n",
    "        },\n",
    "        ytest_roll.index[i*1000:(i+1)*1000],\n",
    "    )\n",
    "    #ytest_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on new subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj2_features, subj2_targets = my_load(subj_ids = [2], series_ids = [x+1 for x in range(8)])\n",
    "subj2_features.shape, subj2_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_subj2 = subj2_features.copy()\n",
    "y_subj2 = subj2_targets.copy()\n",
    "print('x_subj2, y_subj2', x_subj2.shape, y_subj2.shape)\n",
    "\n",
    "xsubj2_roll, ysubj2_roll = preprocess(x_subj2, y_subj2, False)\n",
    "assert xsubj2_roll.shape[0] > 0\n",
    "xsubj2_roll.shape, ysubj2_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 1000*2\n",
    "for i in range(3):\n",
    "    ytest_pred = my_predict(\n",
    "        mod2,\n",
    "        {   'raw_features': xsubj2_roll.values[i*n_step:(i+1)*n_step],\n",
    "            'raw_targets':  ysubj2_roll.values[i*n_step:(i+1)*n_step] + 1,\n",
    "        },\n",
    "        ysubj2_roll.index[i*n_step:(i+1)*n_step],\n",
    "    )\n",
    "    #ytest_pred.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

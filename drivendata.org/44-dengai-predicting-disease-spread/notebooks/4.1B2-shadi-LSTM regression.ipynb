{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Change from 3\n",
    "\n",
    "- LSTM instead of RF\n",
    "- normalizing data to [-1,+1]\n",
    "- 1st attempt was lstm regressor on epidemic_max .. but that only predicted the global average max epidemic\n",
    "- 2nd was to have a more complex architecture\n",
    "  - do LSTM-based AE on raw features except weekofyear\n",
    "  - use encoded features + `is_epidemic` + weekofyear for regression on total_cases\n",
    "  - in prediction, for `is_epidemic`, use the predicted value\n",
    "  - In training, the computed value from the correlation with the half-sin is used\n",
    "\n",
    "  \n",
    "For reference, check the published [benchmark](https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb) with score = 26\n",
    "\n",
    "\n",
    "submissions\n",
    "- 4.1B1 uses the 2nd network\n",
    "  - this got a whooping score of 20.9\n",
    "  - network was trained till epoch 300\n",
    "- 4.1B2 ditto but trained further\n",
    "  - till epoch 600, a bit of difference from 300 submission\n",
    "  - till epoch 900, more improvement\n",
    "  - till epoch 1500, tighter epidemic in sj end-2010\n",
    "  - fix lstm decoder to be of size 15 like encoder\n",
    "    - reconstructed signal matches almost perfectly\n",
    "    - predicted target from training matches all except the high peaks\n",
    "  - added dropout=0.2 on lstm decoder\n",
    "    - submission prediction had epidemics during winter .. bad\n",
    "  - feed raw features without deseasoning .. much better than with deseasoned data\n",
    "  - lahead = 10 -> 60 .. got worse\n",
    "  - add embedding to meta features, with weekofyear not min/max anymore\n",
    "    - stick with lahead=10 for practicality\n",
    "    - result LGTM at epoch 721\n",
    "  - still training till 1100\n",
    "  - score 30!!!\n",
    "    - maybe because of the jumps around epidemic times :s\n",
    "    - maybe should have stopped at epoch 30?\n",
    "- 4.1B3\n",
    "  - train only till epoch 300\n",
    "  - go back to deseasoned\n",
    "  - TODO stop using is_epidemic since prediction is not so good and training is thinking that it's reliable\n",
    "  - TODO go back to no deseasoning since dropped using is_epidemic prediction\n",
    "  - TODO lstm decoder dropout = 0\n",
    "  - TODO lstm decoder size 5\n",
    "  - TODO embedding dimensions too big?\n",
    "  - TODO weekofyear to \"season\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Dropout, Embedding, Flatten\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, RepeatVector, TimeDistributed, Concatenate\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 0.2 data\n",
    "df_is_epidemic = pd.read_pickle('data/processed/0.2A-is_epidemic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 5.1 data\n",
    "df_targ = pd.read_pickle('data/processed/5.1B-df_targ.pkl')\n",
    "df_feat_2 = pd.read_pickle('data/processed/5.1B-df_feat_2.pkl')\n",
    "df_meta = pd.read_pickle('data/processed/5.1B-df_meta.pkl')\n",
    "\n",
    "# match indeces\n",
    "df_meta = df_meta.loc[df_targ.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ.shape, df_is_epidemic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note automatic index matching eventhough not same dimensions\n",
    "df_targ['is_epidemic'] = df_is_epidemic['is_epidemic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ.tail(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_epidemic.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all that make sense\n",
    "selected_features = [x for x in df_feat_2.columns if\n",
    "                     # (x.endswith('_trend') and not x.startswith('weekofyear')) or x=='weekofyear_original'\n",
    "                     # x.endswith('_original') and not x.startswith('weekofyear')\n",
    "                     x.endswith('_trend') and not x.startswith('weekofyear')\n",
    "                    ]\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lahead = 10 # 60 yields no classification results\n",
    "batch_size = 16 # smaller batches lead to less loss of data when truncating non-multiples of batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create rolling windows for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_group(group, n_back):\n",
    "    out = []\n",
    "    for i in range(n_back):\n",
    "        out.append(group.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[(n_back-1):, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    out = np.flip(out, axis=1) # so that the index=0 is the oldest, and index=4 is latest\n",
    "    return out\n",
    "\n",
    "stride_group_2 = lambda x: stride_group(x, lahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop 1st x rows if they are not a multiple of batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_truncate(df):\n",
    "    return (df.groupby(level='city', as_index=False)\n",
    "              .apply(lambda group: group.tail(group.shape[0] - (group.shape[0]%batch_size)))\n",
    "              .reset_index(level=0, drop=True)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_feat_2.loc[~df_meta['submit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_retrain = df_train[selected_features].copy()\n",
    "y_retrain = df_targ[~df_meta['submit']].copy()\n",
    "y_retrain['is_epidemic'] = y_retrain['is_epidemic'].astype('int') # [['total_cases']]\n",
    "x_retrain.shape, y_retrain.shape, y_retrain.groupby('city').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain['is_epidemic'].loc['sj'].plot(label='sj')\n",
    "(y_retrain['is_epidemic']+1.2).loc['iq'].plot(label='iq+1.2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## label each epidemic event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain['epidemic_id'] = ((y_retrain['is_epidemic'].astype('int').diff()+1)//2).fillna(value=0).cumsum(axis=0)\n",
    "y_retrain.loc[~y_retrain['is_epidemic'].astype(bool), 'epidemic_id'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain[~y_retrain['is_epidemic'].astype(bool)].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain[ y_retrain['is_epidemic'].astype(bool)].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_retrain[ y_retrain['is_epidemic'].astype(bool)].tail(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain.groupby('epidemic_id').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xretrain_roll = x_retrain.groupby(level='city').apply(stride_group_2)\n",
    "\n",
    "# drop lahead per city\n",
    "yretrain_roll = (y_retrain\n",
    "                 .groupby(level='city', as_index=False)\n",
    "                 .apply(lambda group: group.iloc[(lahead-1):])\n",
    "                 .reset_index(level=0, drop=True)\n",
    "                )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# filter for is_epidemic = True\n",
    "for city in ['sj','iq']:\n",
    "    xretrain_roll.loc[city] = xretrain_roll.loc[city][yretrain_roll.loc[city,'is_epidemic'].astype('bool')]\n",
    "    \n",
    "yretrain_roll = yretrain_roll[yretrain_roll['is_epidemic'].astype('bool')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-batchsize-multiple per city\n",
    "for city in ['sj','iq']:\n",
    "    to_drop = xretrain_roll.loc[city].shape[0]%batch_size\n",
    "    print('drop non-multiple', city, to_drop)\n",
    "    xretrain_roll.loc[city] = xretrain_roll.loc[city][(to_drop):]\n",
    "    \n",
    "yretrain_roll = my_truncate(yretrain_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xretrain_roll.loc['sj'].shape, xretrain_roll.loc['iq'].shape, yretrain_roll.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate epidemic max amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yretrain_epidemicmax = (\n",
    "    yretrain_roll[['epidemic_id', 'total_cases']]\n",
    "    .groupby('epidemic_id')\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'total_cases': 'epidemic_max'})\n",
    ")\n",
    "yretrain_epidemicmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yretrain_roll = yretrain_roll.reset_index().merge(\n",
    "    yretrain_epidemicmax,\n",
    "    on = 'epidemic_id',\n",
    "    how='left'\n",
    ").set_index(['city', 'week_start_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yretrain_roll.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yretrain_roll[yretrain_roll['epidemic_id']==2].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ~~fit model: lstm regression on epidemic_max or total_cases~~\n",
    "\n",
    "Both only detected the global average"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(5,\n",
    "              input_shape=(lahead, len(selected_features)),\n",
    "              batch_size=batch_size,\n",
    "              activation='linear'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense( 5, activation='relu'))\n",
    "    # model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.add(Lambda(lambda x: x*10))\n",
    "    model.add(Dropout(0.2))\n",
    "    # model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# initialize\n",
    "mod1 = {}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    #if city=='sj': continue # FIXME fitting sj model\n",
    "    mod1[city] = create_model()\n",
    "    #if city=='iq': continue # FIXME skipping iq model\n",
    "    mod1[city].summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# actual fit\n",
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod1[city].fit(\n",
    "             xretrain_roll.loc[city][yretrain_roll['is_epidemic']],\n",
    "             yretrain_roll.loc[city, 'epidemic_max'][yretrain_roll['is_epidemic']], # total_cases\n",
    "             batch_size=batch_size,\n",
    "             epochs=250, #250, #500, # 1000,\n",
    "             verbose=2,\n",
    "             #validation_data=None,\n",
    "             shuffle=False\n",
    "        )\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    #plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title(city)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trained result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def my_predict(city, np_in, index):\n",
    "    print(city, np_in.shape)\n",
    "    np_pred = mod1[city].predict(np_in, batch_size=batch_size)\n",
    "    print(np_pred.shape)\n",
    "    out = pd.DataFrame({\n",
    "        'epidemic_max': np_pred.squeeze(), \n",
    "        'city': city, \n",
    "        'week_of_year': index,\n",
    "    }).set_index(['city', 'week_of_year'])\n",
    "    return out\n",
    "\n",
    "yretrain_pred = pd.concat([\n",
    "    my_predict(\n",
    "        city, \n",
    "        xretrain_roll.loc[city][yretrain_roll.loc[city, 'is_epidemic'].astype('bool')], \n",
    "        yretrain_roll.loc[city][yretrain_roll.loc[city, 'is_epidemic'].astype('bool')].index\n",
    "    )\n",
    "    for city in ['sj','iq']\n",
    "], axis=0)\n",
    "\n",
    "# reverse log10 transform\n",
    "# y_pred['total_cases'] = ((10**((y_pred['total_cases']).clip(upper=3)))-1).astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yretrain_pred.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for city in ['sj','iq']:\n",
    "    (yretrain_pred.loc[city]['epidemic_max']).plot(label='predicted', style='.')\n",
    "    # epidemic_max\n",
    "    yretrain_roll.loc[city]['total_cases'].astype('int').plot(label='actual', figsize=(20,3), style='.')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model: AE coupled with regression on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coupled():\n",
    "    lstm_dim_1 = 15\n",
    "    len_feat = len(selected_features)\n",
    "    input_shape = (lahead, len_feat, )\n",
    "\n",
    "    # inputs\n",
    "    feat_raw = Input(shape=input_shape, name='raw_features')\n",
    "    \n",
    "    # encoder\n",
    "    feat_enc = feat_raw\n",
    "    feat_enc = LSTM(\n",
    "              lstm_dim_1,\n",
    "              #input_shape=input_shape,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=False,\n",
    "              activation='tanh',\n",
    "              name='encoded_features')(feat_enc)\n",
    "\n",
    "    # decoder\n",
    "    feat_rec = feat_enc\n",
    "    feat_rec = RepeatVector(lahead, input_shape=(lstm_dim_1, ))(feat_rec)\n",
    "    feat_rec = LSTM(lstm_dim_1,\n",
    "              #input_shape=(lahead, len(selected_features)),\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=True,\n",
    "              dropout=0.2,\n",
    "              activation='tanh')(feat_rec)\n",
    "    feat_rec = TimeDistributed(\n",
    "        Dense(len_feat, activation='linear'),\n",
    "        name='reconstructed_features'\n",
    "    )(feat_rec)\n",
    "\n",
    "    # append to encoded features\n",
    "    # 2 meta features: is_epidemic and weekofyear\n",
    "    is_epidemic = Input(shape=(1, ), name='is_epidemic')\n",
    "    \"\"\"\n",
    "    embed_epi = is_epidemic\n",
    "    # 2 is vocabulary length, i.e. (0,1)\n",
    "    # 4 is dimensions to use in embedding\n",
    "    embed_epi = Embedding(2, 4, input_length=1, name='embed_epi_matrix')(embed_epi)\n",
    "    embed_epi = Flatten(name='embed_epi_flat')(embed_epi)\n",
    "    \"\"\"\n",
    "\n",
    "    weekofyear = Input(shape=(1, ), name='weekofyear')\n",
    "    embed_woy = weekofyear\n",
    "    # 53+1 is vocabulary length ... remember that weekofyear is not 0-based\n",
    "    # 4 is dimensions to use in embedding\n",
    "    embed_woy = Embedding(53+1, 4, input_length=1, name='embed_woy_matrix')(embed_woy)\n",
    "    embed_woy = Flatten(name='embed_woy_flat')(embed_woy)\n",
    "\n",
    "    feat_enc_and_meta = Concatenate(axis=-1, name='enc_and_meta')([feat_enc, embed_woy]) # embed_epi\n",
    "\n",
    "    # regressor\n",
    "    out = feat_enc_and_meta # feat_enc\n",
    "    out = Dense(5, activation='relu')(out)\n",
    "    out = Dense(1, activation='linear')(out)\n",
    "    out = Lambda(lambda x: x*10, name='regressed_output')(out)\n",
    "    \n",
    "    # create model\n",
    "    model_all = Model(inputs = [feat_raw, is_epidemic, weekofyear], outputs = [feat_rec, out])\n",
    "    model_all.compile(loss='mae', optimizer='adam')\n",
    "    return model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "mod2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    #if city=='sj': continue # FIXME fitting sj model\n",
    "    mod2[city] = create_coupled()\n",
    "    #if city=='iq': continue # FIXME skipping iq model\n",
    "    mod2[city].summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# actual fit\n",
    "for city in ['sj', 'iq']:\n",
    "    # if city=='iq': continue # FIXME skipping iq for now\n",
    "    print(city)\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod2[city].fit(\n",
    "             {   # ...[yretrain_roll['is_epidemic']], to only train on subset of epidemics\n",
    "                 'raw_features': xretrain_roll.loc[city],\n",
    "                 #[yretrain_roll['is_epidemic']],\n",
    "                 # 'epidemic_max'\n",
    "                 'is_epidemic': yretrain_roll.loc[city, ['is_epidemic']],\n",
    "                 'weekofyear': yretrain_roll.loc[city, ['weekofyear']],\n",
    "             },\n",
    "             {   'reconstructed_features': xretrain_roll.loc[city], #[yretrain_roll['is_epidemic']],\n",
    "                 'regressed_output': yretrain_roll.loc[city, 'total_cases'], #[yretrain_roll['is_epidemic']], # epidemic_max\n",
    "             },\n",
    "             batch_size=batch_size,\n",
    "             epochs=300, #250, #500, # 1000,\n",
    "             initial_epoch = 200,\n",
    "             verbose=2,\n",
    "             #validation_data=None,\n",
    "             shuffle=False\n",
    "        )\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    # ignore first few points since large relative to others\n",
    "    plt.plot(history.history['loss'][5:], label='loss')\n",
    "    #plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title(city)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    mod2[city].save('data/processed/4.1B-model-epoch_1500-%s.h5'%city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def my_predict(city, np_in, index):\n",
    "    np_rec, np_pred = mod2[city].predict(np_in, batch_size=batch_size)\n",
    "    \n",
    "    for feat_int in range(len(selected_features)):\n",
    "        pd.DataFrame({\n",
    "            'actual': pd.Series(np_in['raw_features'][:,0,feat_int], index=index),\n",
    "            'pred': pd.Series(np_rec[:,0,feat_int],                index=index),\n",
    "        }).plot(figsize=(20,3))\n",
    "        plt.title('%s / feat %i:'%(city, feat_int))\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "    out = pd.DataFrame({\n",
    "        'epidemic_max': np_pred.squeeze(), \n",
    "        'city': city, \n",
    "        'week_start_date': index,\n",
    "    }).set_index(['city', 'week_start_date'])\n",
    "    return out\n",
    "\n",
    "yretrain_pred = pd.concat([\n",
    "    my_predict(\n",
    "        city, \n",
    "        {   'raw_features': xretrain_roll.loc[city],#[yretrain_roll.loc[city, 'is_epidemic'].astype('bool')], \n",
    "            'is_epidemic': yretrain_roll.loc[city, ['is_epidemic']],\n",
    "            'weekofyear':  yretrain_roll.loc[city, ['weekofyear']],\n",
    "        },\n",
    "        yretrain_roll.loc[city].index,#[yretrain_roll.loc[city, 'is_epidemic'].astype('bool')].index\n",
    "    )\n",
    "    for city in ['sj','iq']\n",
    "], axis=0)\n",
    "\n",
    "# reverse log10 transform\n",
    "# y_pred['total_cases'] = ((10**((y_pred['total_cases']).clip(upper=3)))-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    (yretrain_pred.loc[city]['epidemic_max']).plot(label='predicted', style='.')\n",
    "    # epidemic_max\n",
    "    yretrain_roll.loc[city]['total_cases'].astype('int').plot(label='actual', figsize=(20,3), style='.')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load predicted `is_epidemic` for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isepi_pred = pd.read_pickle('data/processed/4.1A-ysubmit_pred.pkl')\n",
    "# fix index name\n",
    "isepi_pred = isepi_pred.reset_index().rename(columns={'week_of_year': 'week_start_date'})\n",
    "# append weekofyear\n",
    "df_dates = df_targ.reset_index()[['week_start_date','weekofyear']]\n",
    "df_dates = df_dates[~df_dates.duplicated()]\n",
    "isepi_pred = isepi_pred.merge(df_dates, how='left', on='week_start_date')\n",
    "# set index again\n",
    "isepi_pred = isepi_pred.set_index(['city', 'week_start_date'])\n",
    "# threshold probability\n",
    "isepi_pred['is_epidemic'] = isepi_pred['is_epidemic'].apply(lambda x: x>=0.5).astype('int')\n",
    "\n",
    "isepi_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict `is_epidemic` on submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_submit = df_feat_2.loc[ df_meta['submit'], selected_features].copy()\n",
    "\n",
    "xsubmit_roll = x_submit.groupby(level='city').apply(stride_group_2)\n",
    "\n",
    "# drop non-batch_size multiple\n",
    "for city in ['sj','iq']:\n",
    "    to_drop = xsubmit_roll.loc[city].shape[0]%batch_size\n",
    "    print('non multiple', city, to_drop)\n",
    "    xsubmit_roll.loc[city] = xsubmit_roll.loc[city][to_drop:]\n",
    "    \n",
    "# choose any field from x_submit just to get the index\n",
    "ysubmit_roll = (x_submit[x_submit.columns[:1]]\n",
    "                 .groupby(level='city', as_index=False)\n",
    "                 .apply(lambda group: group.iloc[(lahead-1):])\n",
    "                 .reset_index(level=0, drop=True)\n",
    "                *0\n",
    "                )    \n",
    "ysubmit_roll = my_truncate(ysubmit_roll)\n",
    "\n",
    "#  get the is_epidemic prediction, for the same index as above\n",
    "isepipred_roll = isepi_pred.loc[ysubmit_roll.index]\n",
    "\n",
    "x_submit.shape, xsubmit_roll.loc['sj'].shape, xsubmit_roll.loc['iq'].shape, ysubmit_roll.shape, isepi_pred.shape, isepipred_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ysubmit_pred = []\n",
    "for city in ['sj','iq']:\n",
    "    print('shapes', isepipred_roll.loc[city].shape, xsubmit_roll.loc[city][:,-1:,0].shape)\n",
    "    in_1 = {\n",
    "        'raw_features': xsubmit_roll.loc[city],#[yretrain_roll.loc[city, 'is_epidemic'].astype('bool')], \n",
    "        'is_epidemic': isepipred_roll.loc[city, ['is_epidemic']],\n",
    "        'weekofyear': isepipred_roll.loc[city, ['weekofyear']],\n",
    "    }\n",
    "    #[yretrain_roll.loc[city, 'is_epidemic'].astype('bool')].index\n",
    "    res = my_predict(city, in_1, ysubmit_roll.loc[city].index)\n",
    "    ysubmit_pred.append(res)\n",
    "\n",
    "ysubmit_pred = pd.concat(ysubmit_pred, axis=0)\n",
    "\n",
    "# reverse log10 transform\n",
    "# y_pred['total_cases'] = ((10**((y_pred['total_cases']).clip(upper=3)))-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    (ysubmit_pred.loc[city]['epidemic_max']).plot(figsize=(20,3), label=city)\n",
    "\n",
    "plt.title('submission')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    isepi_pred.loc[city, 'is_epidemic'].plot(figsize=(20,3), label=city)\n",
    "\n",
    "plt.title('is_epidemic')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import load_raw\n",
    "df_all = load_raw()\n",
    "\n",
    "submit = df_all['submission'].copy()\n",
    "# TODO if this matches indeces properly, review the complicated merge in 3.1\n",
    "submit['total_cases'] = ysubmit_pred['epidemic_max']\n",
    "submit = submit.fillna(value=0)\n",
    "submit['total_cases'] = submit['total_cases'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    submit.loc[city, 'total_cases'].plot(label=city, figsize=(20,3))\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import make_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(submit.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import load_raw, imply_columns\n",
    "\n",
    "df = load_raw()\n",
    "\n",
    "from src.features import append_features\n",
    "\n",
    "df['train'], df['submit'] = map(append_features, [df['train'], df['submit']])\n",
    "\n",
    "cols = imply_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train'].shape, df['train'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['train'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['submit'].shape, df['submit'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['submit'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size=0. # 0.3 FIXME\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(df['train'][cols['features']], df['train'][cols['target']], test_size=test_size)\n",
    "x_train.shape, x_valid.shape, y_train.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.values\n",
    "y_train = y_train.squeeze()\n",
    "x_submit = df['submit'][cols['features']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess with Random Trees Embedding\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "rte = RandomTreesEmbedding(\n",
    "    n_estimators=100, \n",
    "    max_depth=50, \n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    min_impurity_split=None, \n",
    "    sparse_output=True, \n",
    "    n_jobs=1,\n",
    "    random_state=None, \n",
    "    verbose=0, \n",
    "    warm_start=False\n",
    ")\n",
    "x_train = rte.fit(x_train)\n",
    "\n",
    "preprocess = lambda x_in: rte.transform(x_in)\n",
    "x_train, x_submit = map(preprocess, [x_train, x_submit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## auto-sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autosklearn.classification\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-sklearn API\n",
    "# http://automl.github.io/auto-sklearn/dev/api.html\n",
    "#\n",
    "# Cross-validation from\n",
    "# https://github.com/automl/auto-sklearn/blob/master/example/example_crossvalidation.py\n",
    "\n",
    "automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "    time_left_for_this_task=30*60,\n",
    "    per_run_time_limit=30,\n",
    "    resampling_strategy='cv',\n",
    "    resampling_strategy_arguments={'folds': 3}\n",
    ")\n",
    "\n",
    "# raw data\n",
    "import time\n",
    "print(time.ctime(), 'start fit')\n",
    "automl.fit(x_train.copy(), y_train.copy())\n",
    "print(time.ctime(), 'start refit')\n",
    "automl.refit(x_train.copy(), y_train.copy())\n",
    "print(time.ctime(), 'end')\n",
    "    \n",
    "# log(x+1)\n",
    "# automl.fit(np.log(x_train.values+1), y_train.squeeze())\n",
    "\n",
    "print(automl.sprint_statistics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_valid = x_train\n",
    "y_valid = y_train\n",
    "x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data/\n",
    "y_hat = automl.predict(x_valid)\n",
    "y_prob = automl.predict_proba(x_valid)\n",
    "\n",
    "# log(x+1)\n",
    "# y_hat = automl.predict(np.log(x_valid.values+1))\n",
    "# y_prob = automl.predict_proba(np.log(x_valid.values+1))\n",
    "\n",
    "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_valid, y_hat))\n",
    "\n",
    "y_prob = y_prob[:,automl._automl._classes[0] == 1]\n",
    "\n",
    "print(\"Log loss\", sklearn.metrics.log_loss(y_valid, y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(automl.show_models())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a submission from automl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = automl.predict_proba(df['submit'][cols['features']].values)\n",
    "df['submit'][cols['target'][0]] = y_pred[:,automl._automl._classes[0] == 1]\n",
    "\n",
    "# test[cols_target[0]].head().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['upload'] = df['submit'][['Unnamed: 0', cols['target'][0]]].rename(columns={'Unnamed: 0': ''})\n",
    "df['upload'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features import make_submission\n",
    "make_submission(df['upload'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## statsmodels GLM\n",
    "\n",
    "- Idea from https://github.com/jthalstead/DrivenData---Blood-Donations/blob/master/blood_single_glm.R\n",
    "- by user https://www.drivendata.org/users/jackh/\n",
    "- also, all the 0.1311 submissions are with the hack of using an external dataset from UCI, as listed [here](https://community.drivendata.org/t/using-uci-data-to-achieve-0-1311-on-lb-just-for-fun-here/883)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.dtype, y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "gamma_model = sm.GLM(x_train, y_train, family=sm.families.Gamma())\n",
    "gamma_results = gamma_model.fit()\n",
    "\n",
    "print(gamma_results.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

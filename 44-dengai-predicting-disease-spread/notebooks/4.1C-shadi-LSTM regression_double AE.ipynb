{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change from 4.1B\n",
    "- ~~go back to no deseasoning since dropped using is_epidemic prediction~~\n",
    "    - current model detects huge total cases peak in training when deseasoning is fed into it\n",
    "    - sticking to deseasoned\n",
    "    - FIXME maybe this is due to the last Dense layer being only of size 5\n",
    "- ~~weekofyear to \"season\"~~\n",
    "    - resulted in step-wise prediction\n",
    "    - FIXME turns out that the embedding layer had \"vocab size\" = 53 despite seasons having cardinality = 4\n",
    "- AE on target too ... like painting style transfer to content\n",
    "- TODO lstm decoder dropout = 0\n",
    "- TODO lstm decoder size back to 5\n",
    "- TODO embedding dimensions too big?\n",
    "\n",
    "\n",
    "Submissions\n",
    "- 4.1C1 .. without clipping negatives, trained till 600\n",
    "- 4.1C2 .. trained till 600, with clipping negatives .. score 22\n",
    "- 4.1C3 .. training till 1400\n",
    "  - Remember this had an internal Dense(100) and AE-LSTM(15) layers\n",
    "  - score .. 18\n",
    "- 4.1C4 trained till 2000 .. score 19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Dropout, Embedding, Flatten, Subtract, Dot\n",
    "\n",
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, RepeatVector, TimeDistributed, Concatenate\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use 0.2 data\n",
    "df_is_epidemic = pd.read_pickle('data/processed/0.2A-is_epidemic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load 5.1 data\n",
    "df_targ = pd.read_pickle('data/processed/5.1B-df_targ.pkl')\n",
    "df_feat_2 = pd.read_pickle('data/processed/5.1B-df_feat_2.pkl')\n",
    "df_meta = pd.read_pickle('data/processed/5.1B-df_meta.pkl')\n",
    "\n",
    "# match indeces\n",
    "df_meta = df_meta.loc[df_targ.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ.shape, df_is_epidemic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note automatic index matching eventhough not same dimensions\n",
    "df_targ['is_epidemic'] = df_is_epidemic['is_epidemic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ.tail(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_is_epidemic.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all that make sense\n",
    "selected_features = [x for x in df_feat_2.columns if\n",
    "                     # (x.endswith('_trend') and not x.startswith('weekofyear')) or x=='weekofyear_original'\n",
    "                     # x.endswith('_original') and not x.startswith('weekofyear')\n",
    "                     x.endswith('_trend') and not x.startswith('weekofyear')\n",
    "                    ]\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lahead = 10 # 60 yields no classification results\n",
    "batch_size = 16 # smaller batches lead to less loss of data when truncating non-multiples of batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create rolling windows for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_group(group, n_back):\n",
    "    out = []\n",
    "    for i in range(n_back):\n",
    "        out.append(group.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[(n_back-1):, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    out = np.flip(out, axis=1) # so that the index=0 is the oldest, and index=4 is latest\n",
    "    return out\n",
    "\n",
    "stride_group_2 = lambda x: stride_group(x, lahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop 1st x rows if they are not a multiple of batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_truncate(df):\n",
    "    return (df.groupby(level='city', as_index=False)\n",
    "              .apply(lambda group: group.tail(group.shape[0] - (group.shape[0]%batch_size)))\n",
    "              .reset_index(level=0, drop=True)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare data to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_feat_2.loc[~df_meta['submit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_retrain = df_train[selected_features].copy()\n",
    "y_retrain = df_targ[~df_meta['submit']].copy()\n",
    "y_retrain['is_epidemic'] = y_retrain['is_epidemic'].astype('int') # [['total_cases']]\n",
    "x_retrain.shape, y_retrain.shape, y_retrain.groupby('city').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain['is_epidemic'].loc['sj'].plot(label='sj')\n",
    "(y_retrain['is_epidemic']+1.2).loc['iq'].plot(label='iq+1.2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain['weekofyear'].loc['sj'].plot(label='sj')\n",
    "(y_retrain['weekofyear']+1.2).loc['iq'].plot(label='iq+1.2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess the weekofyear to \"season\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# approximate weekofyear to season mapping\n",
    "season_week = [\n",
    "    # season: end week number (wraps at 53)\n",
    "    ('winter', 11),\n",
    "    ('spring', 24),\n",
    "    ('summer', 37),\n",
    "    ('fall', 50),\n",
    "    ('winter', 53),\n",
    "]\n",
    "def weekofyear_to_season(weekofyear):\n",
    "    for season, woy_start in season_week:\n",
    "        if weekofyear <= woy_start: return season\n",
    "\n",
    "weekofyear_to_season(25)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "y_retrain['season_str'] = y_retrain['weekofyear'].apply(weekofyear_to_season)\n",
    "y_retrain['season_int'] = pd.factorize(y_retrain['season_str'])[0]\n",
    "y_retrain.groupby('season_str').head(n=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "woy_season_map = y_retrain[['season_str', 'season_int']]\n",
    "woy_season_map = woy_season_map[~woy_season_map.duplicated()].reset_index(drop=True).set_index('season_str')\n",
    "woy_season_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess total_cases so it can be [0,1] for input into AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yretrain_max = y_retrain['total_cases'].groupby('city').max()\n",
    "yretrain_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain['norm_cases'] = y_retrain['total_cases'].groupby('city').apply(lambda x: x / x.max())\n",
    "\n",
    "y_retrain['norm_cases'].groupby('city').max(), y_retrain['norm_cases'].groupby('city').min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_retrain.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xretrain_roll = x_retrain.groupby(level='city').apply(stride_group_2)\n",
    "yretrain_roll = y_retrain[['norm_cases',]].groupby(level='city').apply(stride_group_2)\n",
    "\n",
    "# \"meta\" dataframe\n",
    "zretrain_roll = (y_retrain\n",
    "                 .groupby(level='city', as_index=False)\n",
    "                 .apply(lambda group: group.iloc[(lahead-1):])\n",
    "                 .reset_index(level=0, drop=True)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop non-batchsize-multiple per city\n",
    "for city in ['sj','iq']:\n",
    "    to_drop = xretrain_roll.loc[city].shape[0]%batch_size\n",
    "    print('drop non-multiple', city, to_drop)\n",
    "    xretrain_roll.loc[city] = xretrain_roll.loc[city][(to_drop):]\n",
    "    yretrain_roll.loc[city] = yretrain_roll.loc[city][(to_drop):]\n",
    "    \n",
    "zretrain_roll = my_truncate(zretrain_roll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( xretrain_roll.loc['sj'].shape, xretrain_roll.loc['iq'].shape, \n",
    "  yretrain_roll.loc['sj'].shape, yretrain_roll.loc['iq'].shape, \n",
    "  zretrain_roll.loc['sj'].shape, zretrain_roll.loc['iq'].shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit model: AE coupled with regression on target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coupled():\n",
    "    lstm_dim_1 = 15\n",
    "    len_feat = len(selected_features)\n",
    "    input_shape = (lahead, len_feat, )\n",
    "\n",
    "    # inputs\n",
    "    feat_raw = Input(shape=input_shape, name='raw_features')\n",
    "    targ_raw = Input(shape=(lahead, 1, ), name='raw_targets')\n",
    "    \n",
    "    # features encoder\n",
    "    feat_enc = feat_raw\n",
    "    feat_enc = LSTM(\n",
    "              lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=False,\n",
    "              activation='tanh',\n",
    "              name='encoded_features')(feat_enc)\n",
    "\n",
    "    # features decoder\n",
    "    feat_rec = feat_enc\n",
    "    feat_rec = RepeatVector(lahead, input_shape=(lstm_dim_1, ))(feat_rec)\n",
    "    feat_rec = LSTM(lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=True,\n",
    "              dropout=0.2,\n",
    "              activation='tanh')(feat_rec)\n",
    "    feat_rec = TimeDistributed(\n",
    "        Dense(len_feat, activation='linear'),\n",
    "        name='reconstructed_features'\n",
    "    )(feat_rec)\n",
    "\n",
    "    # target encoder\n",
    "    targ_enc = targ_raw\n",
    "    targ_enc = LSTM(\n",
    "              lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=False,\n",
    "              activation='tanh',\n",
    "              name='encoded_targets')(targ_enc)\n",
    "\n",
    "    # target decoder\n",
    "    targ_rec = targ_enc\n",
    "    targ_rec = RepeatVector(lahead, input_shape=(lstm_dim_1, ), name='targ_dec_1')(targ_rec)\n",
    "    targ_rec = LSTM(lstm_dim_1,\n",
    "              batch_size=batch_size,\n",
    "              return_sequences=True,\n",
    "              dropout=0.2,\n",
    "              activation='tanh', name='targ_dec_2')(targ_rec)\n",
    "    targ_rec = TimeDistributed(\n",
    "        Dense(1, activation='linear'),\n",
    "        name='reconstructed_targets'\n",
    "    )(targ_rec)\n",
    "\n",
    "    # append to encoded features\n",
    "    # 2 meta features: is_epidemic and weekofyear\n",
    "    \"\"\"\n",
    "    is_epidemic = Input(shape=(1, ), name='is_epidemic')\n",
    "    embed_epi = is_epidemic\n",
    "    # 2 is vocabulary length, i.e. (0,1)\n",
    "    # 4 is dimensions to use in embedding\n",
    "    embed_epi = Embedding(2, 4, input_length=1, name='embed_epi_matrix')(embed_epi)\n",
    "    embed_epi = Flatten(name='embed_epi_flat')(embed_epi)\n",
    "    \"\"\"\n",
    "\n",
    "    weekofyear = Input(shape=(1, ), name='weekofyear')\n",
    "    embed_woy = weekofyear\n",
    "    # 53+1 is vocabulary length ... remember that weekofyear is not 0-based\n",
    "    # 4 is dimensions to use in embedding\n",
    "    embed_woy = Embedding(53+1, 4, input_length=1, name='embed_woy_matrix')(embed_woy)\n",
    "    embed_woy = Flatten(name='embed_woy_flat')(embed_woy)\n",
    "\n",
    "    feat_enc_and_meta = Concatenate(axis=-1, name='enc_and_meta')([feat_enc, embed_woy]) # embed_epi\n",
    "\n",
    "    # regressor\n",
    "    out = feat_enc_and_meta # feat_enc\n",
    "    # out = Dense(lstm_dim_1 + 4, activation='relu', name='pre_targ_dec_1')(out) # match shape of feat_enc_and_meta\n",
    "    out = Dense(100           , activation='relu', name='pre_targ_dec_1')(out)\n",
    "    out = Dense(lstm_dim_1    , activation='linear', name='pre_targ_dec_2')(out) # match shape of targ_enc\n",
    "    \n",
    "    # subtract the prediction of the encoded features from the encoded target\n",
    "    to_be_zero = Subtract()([out, targ_enc])\n",
    "    to_be_zero = Dot(axes=-1, name='regressed_output')([to_be_zero, to_be_zero])\n",
    "\n",
    "    # create model\n",
    "    # model_all = Model(inputs = [feat_raw, is_epidemic, weekofyear], outputs = [feat_rec, out])\n",
    "    model_all = Model(inputs = [feat_raw, targ_raw, weekofyear], outputs = [feat_rec, targ_rec, to_be_zero])\n",
    "    model_all.compile(loss='mae', optimizer='adam')\n",
    "    return model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "mod2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    #if city=='sj': continue # FIXME fitting sj model\n",
    "    mod2[city] = create_coupled()\n",
    "    #if city=='iq': continue # FIXME skipping iq model\n",
    "    mod2[city].summary()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# actual fit\n",
    "for city in ['sj', 'iq']:\n",
    "    # if city=='sj': continue # FIXME skipping city\n",
    "    print(city)\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod2[city].fit(\n",
    "             {   # ...[yretrain_roll['is_epidemic']], to only train on subset of epidemics\n",
    "                 'raw_features': xretrain_roll.loc[city],\n",
    "                 'raw_targets': yretrain_roll.loc[city],\n",
    "                 'weekofyear': zretrain_roll.loc[city, 'weekofyear'],\n",
    "                 # 'weekofyear': yretrain_roll.loc[city, ['season_int']],\n",
    "             },\n",
    "             {   'reconstructed_features': xretrain_roll.loc[city], #[yretrain_roll['is_epidemic']],\n",
    "                 'reconstructed_targets': yretrain_roll.loc[city],\n",
    "                 # 'regressed_output': yretrain_roll.loc[city, 'total_cases'], #[yretrain_roll['is_epidemic']], # epidemic_max\n",
    "                 'regressed_output': yretrain_roll.loc[city][:,-1,0]*0, # zeros\n",
    "             },\n",
    "             batch_size=batch_size,\n",
    "             epochs=2000, #250, #500, # 1000,\n",
    "             initial_epoch = 1400,\n",
    "             verbose=2,\n",
    "             #validation_data=None,\n",
    "             shuffle=False\n",
    "        )\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    # ignore first few points since large relative to others\n",
    "    plt.plot(history.history['loss'][5:], label='loss')\n",
    "    #plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title(city)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    mod2[city].save('data/processed/4.1C-model-epoch_1500-%s.h5'%city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract prediction model from features to target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/keras-team/keras/blob/master/examples/mnist_transfer_cnn.py#L89\n",
    "mod3 = {}\n",
    "for city in ['sj', 'iq']:\n",
    "    # i1 = mod2[city].get_layer(name='raw_features')\n",
    "    i1 = Input(shape=(lahead, len(selected_features)), name='raw_features')\n",
    "    m1 = mod2[city].get_layer(name='encoded_features')(i1)\n",
    "    \n",
    "    # i2 = mod2[city].get_layer(name='weekofyear')\n",
    "    i2 = Input(shape=(1,), name='weekofyear')\n",
    "    m2 = mod2[city].get_layer(name='embed_woy_matrix')(i2)\n",
    "    m2 = mod2[city].get_layer(name='embed_woy_flat')(m2)\n",
    "    \n",
    "    m3 = mod2[city].get_layer(name='enc_and_meta')([m1, m2])\n",
    "    m3 = mod2[city].get_layer(name='pre_targ_dec_1')(m3)\n",
    "    m3 = mod2[city].get_layer(name='pre_targ_dec_2')(m3)\n",
    "    m3 = mod2[city].get_layer(name='targ_dec_1')(m3)\n",
    "    m3 = mod2[city].get_layer(name='targ_dec_2')(m3)\n",
    "    m3 = mod2[city].get_layer(name='reconstructed_targets')(m3)\n",
    "    \n",
    "    mod3[city] = Model(inputs = [i1, i2], outputs = [m3])\n",
    "    mod3[city].compile(loss='mae', optimizer='adam')\n",
    "\n",
    "    mod3[city].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot trained result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def my_predict(city, np_in, index):\n",
    "    \n",
    "    # make prediction\n",
    "    targ_rec = mod3[city].predict(np_in, batch_size=batch_size)\n",
    "\n",
    "    # plot feature reconstruction\n",
    "    #for feat_int in range(len(selected_features)):\n",
    "    #    pd.DataFrame({\n",
    "    #        'actual': pd.Series(np_in['raw_features'][:,0,feat_int], index=index),\n",
    "    #        'pred': pd.Series(feat_rec[:,0,feat_int],                index=index),\n",
    "    #    }).plot(figsize=(20,3))\n",
    "    #    plt.title('%s / feat %i:'%(city, feat_int))\n",
    "    #    plt.legend()\n",
    "    #    plt.show()\n",
    "        \n",
    "    # plot target reconstruction\n",
    "    feat_int = 0\n",
    "    pd.DataFrame({\n",
    "        'actual': pd.Series(np_in['raw_targets'][:,-1,feat_int],  index=index),\n",
    "        'pred': pd.Series(targ_rec[:,-1,feat_int],  index=index),\n",
    "    }).plot(figsize=(20,3))\n",
    "    plt.title('%s / target %i:'%(city, feat_int))\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # prepare output\n",
    "    out = pd.DataFrame({\n",
    "        'epidemic_max': targ_rec[:,-1,0].squeeze(), \n",
    "        'city': city, \n",
    "        'week_start_date': index,\n",
    "    }).set_index(['city', 'week_start_date'])\n",
    "    return out\n",
    "\n",
    "yretrain_pred = pd.concat([\n",
    "    my_predict(\n",
    "        city, \n",
    "        {   'raw_features': xretrain_roll.loc[city],#[yretrain_roll.loc[city, 'is_epidemic'].astype('bool')], \n",
    "            'raw_targets':  yretrain_roll.loc[city],\n",
    "            #'is_epidemic': yretrain_roll.loc[city, ['is_epidemic']],\n",
    "            # 'weekofyear':  yretrain_roll.loc[city, ['weekofyear']],\n",
    "            'weekofyear': zretrain_roll.loc[city, 'weekofyear'],\n",
    "        },\n",
    "        zretrain_roll.loc[city].index,\n",
    "    )\n",
    "    for city in ['sj','iq']\n",
    "], axis=0)\n",
    "\n",
    "# reverse log10 transform\n",
    "# y_pred['total_cases'] = ((10**((y_pred['total_cases']).clip(upper=3)))-1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    (yretrain_pred.loc[city]['epidemic_max']).plot(label='predicted', style='.')\n",
    "    # epidemic_max\n",
    "    pd.Series(\n",
    "        yretrain_roll.loc[city][:,-1,0],\n",
    "        index = zretrain_roll.loc[city].index\n",
    "    ).plot(label='actual', figsize=(20,3), style='.')\n",
    "    plt.legend()\n",
    "    plt.title(city)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load predicted `is_epidemic` for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isepi_pred = pd.read_pickle('data/processed/4.1A-ysubmit_pred.pkl')\n",
    "# fix index name\n",
    "isepi_pred = isepi_pred.reset_index().rename(columns={'week_of_year': 'week_start_date'})\n",
    "# append weekofyear\n",
    "df_dates = df_targ.reset_index()[['week_start_date','weekofyear']]\n",
    "df_dates = df_dates[~df_dates.duplicated()]\n",
    "isepi_pred = isepi_pred.merge(df_dates, how='left', on='week_start_date')\n",
    "# set index again\n",
    "isepi_pred = isepi_pred.set_index(['city', 'week_start_date'])\n",
    "# threshold probability\n",
    "isepi_pred['is_epidemic'] = isepi_pred['is_epidemic'].apply(lambda x: x>=0.5).astype('int')\n",
    "\n",
    "isepi_pred.head(n=2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# get season\n",
    "isepi_pred['season_str'] = isepi_pred['weekofyear'].apply(weekofyear_to_season)\n",
    "\n",
    "# use same mapping for season string to integer as in training\n",
    "isepi_pred = isepi_pred.reset_index().merge(woy_season_map.reset_index(), how='left', on='season_str').set_index(['city', 'week_start_date'])\n",
    "isepi_pred.head(n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict `is_epidemic` on submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_submit = df_feat_2.loc[ df_meta['submit'], selected_features    ].copy()\n",
    "y_submit = df_feat_2.loc[ df_meta['submit'], selected_features[:1]].copy() # this is a dummy target\n",
    "\n",
    "xsubmit_roll = x_submit.groupby(level='city').apply(stride_group_2)\n",
    "ysubmit_roll = y_submit.groupby(level='city').apply(stride_group_2)\n",
    "\n",
    "# drop non-batch_size multiple\n",
    "for city in ['sj','iq']:\n",
    "    to_drop = xsubmit_roll.loc[city].shape[0]%batch_size\n",
    "    print('non multiple', city, to_drop)\n",
    "    xsubmit_roll.loc[city] = xsubmit_roll.loc[city][to_drop:]\n",
    "    ysubmit_roll.loc[city] = ysubmit_roll.loc[city][to_drop:]\n",
    "    \n",
    "# choose any field from x_submit just to get the index\n",
    "zsubmit_roll = (x_submit[x_submit.columns[:1]]\n",
    "                 .groupby(level='city', as_index=False)\n",
    "                 .apply(lambda group: group.iloc[(lahead-1):])\n",
    "                 .reset_index(level=0, drop=True)\n",
    "                *0\n",
    "                )    \n",
    "zsubmit_roll = my_truncate(zsubmit_roll)\n",
    "\n",
    "#  get the is_epidemic prediction, for the same index as above\n",
    "isepipred_roll = isepi_pred.loc[ysubmit_roll.index]\n",
    "\n",
    "x_submit.shape, xsubmit_roll.loc['sj'].shape, xsubmit_roll.loc['iq'].shape, ysubmit_roll.shape, isepi_pred.shape, isepipred_roll.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysubmit_pred = []\n",
    "for city in ['sj','iq']:\n",
    "    print('shapes', isepipred_roll.loc[city].shape, xsubmit_roll.loc[city][:,-1:,0].shape)\n",
    "    in_1 = {\n",
    "        'raw_features': xsubmit_roll.loc[city],#[yretrain_roll.loc[city, 'is_epidemic'].astype('bool')], \n",
    "        'raw_targets':  ysubmit_roll.loc[city],\n",
    "        #'is_epidemic': isepipred_roll.loc[city, ['is_epidemic']],\n",
    "        'weekofyear': isepipred_roll.loc[city, ['weekofyear']],\n",
    "        # 'weekofyear': isepipred_roll.loc[city, ['season_int']],\n",
    "    }\n",
    "    #[yretrain_roll.loc[city, 'is_epidemic'].astype('bool')].index\n",
    "    res = my_predict(city, in_1, zsubmit_roll.loc[city].index)\n",
    "    ysubmit_pred.append(res)\n",
    "\n",
    "ysubmit_pred = pd.concat(ysubmit_pred, axis=0)\n",
    "\n",
    "ysubmit_pred.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysubmit_pred.groupby('city')['epidemic_max'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reverse log10 transform\n",
    "# y_pred['total_cases'] = ((10**((y_pred['total_cases']).clip(upper=3)))-1).astype(int)\n",
    "\n",
    "# postprocess\n",
    "for city in ['sj','iq']:\n",
    "    # reverse scale\n",
    "    ysubmit_pred.loc[city, 'epidemic_max'] = (ysubmit_pred.loc[city, 'epidemic_max'] * yretrain_max.loc[city]).values\n",
    "    \n",
    "# clip negatives\n",
    "ysubmit_pred['epidemic_max'][ysubmit_pred['epidemic_max']<0] = 0\n",
    "    \n",
    "ysubmit_pred.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ysubmit_pred.groupby('city')['epidemic_max'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    (ysubmit_pred.loc[city]['epidemic_max']).plot(figsize=(20,3), label=city)\n",
    "\n",
    "plt.title('submission')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import load_raw\n",
    "df_all = load_raw()\n",
    "\n",
    "submit = df_all['submission'].copy()\n",
    "# TODO if this matches indeces properly, review the complicated merge in 3.1\n",
    "submit['total_cases'] = ysubmit_pred['epidemic_max']\n",
    "submit = submit.fillna(value=0)\n",
    "submit['total_cases'] = submit['total_cases'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  epoch 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  epoch 2000\n",
    "for city in ['sj','iq']:\n",
    "    submit.loc[city, 'total_cases'].plot(label=city, figsize=(20,3))\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import make_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(submit.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

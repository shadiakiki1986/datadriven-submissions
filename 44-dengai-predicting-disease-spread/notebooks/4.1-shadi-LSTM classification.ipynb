{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change from 3\n",
    "\n",
    "- LSTM instead of RF\n",
    "- normalizing data to [-1,+1]\n",
    "\n",
    "For reference, check https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import load_raw\n",
    "\n",
    "df_all = load_raw()\n",
    "\n",
    "# replace with 0.2 output\n",
    "df_all['labels_train'] = pd.read_pickle('data/processed/is_epidemic.pkl')\n",
    "\n",
    "df_all.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['features_train', 'features_test']:\n",
    "    df_all[k] = df_all[k].groupby('city').apply(lambda group: group.fillna(method='ffill'))\n",
    "    assert ~(pd.isnull(df_all[k]).any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## append without seasonality\n",
    "\n",
    "Copied from notebook 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_diff = 1\n",
    "for k in ['features_train', 'features_test']:\n",
    "    temp_no = (df_all[k]\n",
    "               .groupby('city', as_index=False)\n",
    "               .apply(lambda group: group.diff(periods=n_diff).iloc[n_diff:])\n",
    "               .reset_index(level=0, drop=True)\n",
    "              )\n",
    "    temp_no.columns = [\"%s_diff\"%x for x in temp_no.columns]\n",
    "    assert ~(pd.isnull(temp_no).any().any())\n",
    "    \n",
    "    temp_yes = (df_all[k]\n",
    "               .groupby('city', as_index=False)\n",
    "               .apply(lambda group: group.iloc[n_diff:])\n",
    "               .reset_index(level=0, drop=True)\n",
    "              )\n",
    "    \n",
    "    df_all[k] = pd.concat([temp_yes, temp_no], axis=1)\n",
    "    print(df_all[k].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in ['features_train', 'features_test']:\n",
    "    assert ~(pd.isnull(df_all[k]).any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop n_diff points from labels as well\n",
    "for k in ['labels_train']:\n",
    "    temp_yes = (df_all[k]\n",
    "               .groupby('city', as_index=False)\n",
    "               .apply(lambda group: group.iloc[n_diff:])\n",
    "               .reset_index(level=0, drop=True)\n",
    "              )\n",
    "    \n",
    "    df_all[k] = temp_yes\n",
    "    print(df_all[k].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected from\n",
    "# https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb\n",
    "#selected_features = ['reanalysis_specific_humidity_g_per_kg', \n",
    "#                 'reanalysis_dew_point_temp_k', \n",
    "#                 'station_avg_temp_c', \n",
    "#                 'station_min_temp_c']\n",
    "\n",
    "# all features\n",
    "selected_features = df_all['features_train'].columns\n",
    "\n",
    "# from RF feature importances\n",
    "# selected_features = ['station_max_temp_c', 'reanalysis_dew_point_temp_k',\n",
    "#        'reanalysis_specific_humidity_g_per_kg', 'year', 'weekofyear',\n",
    "#        'ndvi_sw', 'ndvi_se']\n",
    "\n",
    "# from RF with diff\n",
    "# selected_features = ['reanalysis_avg_temp_k_diff', 'station_avg_temp_c', 'ndvi_se_diff',\n",
    "#        'station_max_temp_c', 'reanalysis_dew_point_temp_k',\n",
    "#        'reanalysis_specific_humidity_g_per_kg', 'year', 'weekofyear',\n",
    "#        'ndvi_sw', 'ndvi_se']\n",
    "\n",
    "assert len(set(selected_features) - set(df_all['features_train'].columns))==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['features_train'].shape, df_all['labels_train'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note avoiding class bias\n",
    "x_train = (df_all['features_train']\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.head(n=group.shape[0]*3//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "          )\n",
    "x_test = (df_all['features_train']\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "         )\n",
    "y_train = (df_all['labels_train']\n",
    "          .groupby('city', as_index=False)\n",
    "          .apply(lambda group: group.head(n=group.shape[0]*3//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [['total_cases']]\n",
    "         )\n",
    "y_test = (df_all['labels_train']\n",
    "          .groupby('city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//4))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [['total_cases']]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train.reset_index()['city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize features to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def my_scale(df1):\n",
    "    scaler = MinMaxScaler()\n",
    "    df2 = scaler.fit_transform(df1)\n",
    "    df2 = pd.DataFrame(df2, columns=df1.columns, index=df1.index)\n",
    "    return df2, scaler\n",
    "\n",
    "xtrain_scaled, scaler_xtrain = my_scale(x_train)\n",
    "xtest_scaled, scaler_xtest = my_scale(x_test)\n",
    "#ytrain_scaled, scaler_ytrain = my_scale(y_train)\n",
    "#ytest_scaled, scaler_ytest = my_scale(y_test)\n",
    "ytrain_scaled = y_train\n",
    "scaler_ytrain = None\n",
    "ytest_scaled = y_test\n",
    "scaler_ytest = None\n",
    "\n",
    "xtrain_scaled.shape, xtest_scaled.shape, ytrain_scaled.shape, ytest_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lahead = 10\n",
    "batch_size = 16 # smaller batches lead to less loss of data when truncating non-multiples of batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create rolling windows for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_group(group):\n",
    "    out = []\n",
    "    for i in range(lahead):\n",
    "        out.append(group.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[lahead:, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    out = np.flip(out, axis=1) # so that the index=0 is the oldest, and index=4 is latest\n",
    "    return out\n",
    "\n",
    "    \n",
    "xtrain_roll = xtrain_scaled.groupby(level='city').apply(stride_group)\n",
    "xtest_roll  = xtest_scaled.groupby(level='city').apply(stride_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the target, drop first lahead points, without any rolling window\n",
    "ytrain_roll = (ytrain_scaled\n",
    "                .groupby(level='city', as_index=False)\n",
    "                .apply(lambda group: group.iloc[lahead:])\n",
    "                .reset_index(level=0, drop=True)\n",
    "                )\n",
    "ytest_roll = (ytest_scaled\n",
    "              .groupby(level='city', as_index=False)\n",
    "              .apply(lambda group: group.iloc[lahead:])\n",
    "              .reset_index(level=0, drop=True)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[(city, [df.loc[city].shape for df in (xtrain_roll, xtest_roll, ytrain_roll, ytest_roll)]) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_roll.groupby('city').size(), ytest_roll.groupby('city').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop 1st x rows if they are not a multiple of batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    xtrain_roll.loc[city] = xtrain_roll.loc[city][(xtrain_roll.loc[city].shape[0]%batch_size):]\n",
    "    xtest_roll.loc[city] = xtest_roll.loc[city][(xtest_roll.loc[city].shape[0]%batch_size):]\n",
    "    \n",
    "def my_truncate(df):\n",
    "    return (df.groupby(level='city', as_index=False)\n",
    "              .apply(lambda group: group.tail(group.shape[0] - (group.shape[0]%batch_size)))\n",
    "              .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "ytrain_roll = my_truncate(ytrain_roll)\n",
    "ytest_roll = my_truncate(ytest_roll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_roll.groupby('city').size(), ytest_roll.groupby('city').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[(city, [df.loc[city].shape for df in (xtrain_roll, xtest_roll, ytrain_roll, ytest_roll)]) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify data consistency between raw / scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_scaled.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_scaled.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify data consistency between scaled / roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_scaled.loc['sj'].head(n=24+10).tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_roll.loc['sj'][:3,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_roll.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorize target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_roll['is_epidemic'] = ytrain_roll['total_cases'].apply(lambda x: 1 if x > 10 else 0)\n",
    "ytest_roll['is_epidemic']  = ytest_roll['total_cases'].apply(lambda x: 1 if x > 10 else 0)\n",
    "\n",
    "ytrain_roll.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100,\n",
    "              input_shape=(lahead, len(selected_features)),\n",
    "              batch_size=batch_size,\n",
    "              activation='linear'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = {}\n",
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    mod1[city] = create_model()\n",
    "    mod1[city].summary()\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod1[city].fit(\n",
    "             xtrain_roll.loc[city],\n",
    "             ytrain_roll.loc[city, 'is_epidemic'],\n",
    "             batch_size=batch_size,\n",
    "             epochs=300,\n",
    "             verbose=2,\n",
    "             validation_data=(\n",
    "                 xtest_roll.loc[city],\n",
    "                 ytest_roll.loc[city, 'is_epidemic']\n",
    "             ),\n",
    "             shuffle=False)\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int since we know the label is integer\n",
    "predictions = (ytest_roll[['is_epidemic']].copy()*0).astype('int')\n",
    "\n",
    "predictions.loc['sj', 'is_epidemic'] = mod1['sj'].predict(xtest_roll.loc['sj'], batch_size=batch_size)#.astype(int)\n",
    "predictions.loc['iq', 'is_epidemic'] = mod1['iq'].predict(xtest_roll.loc['iq'], batch_size=batch_size)#.astype(int)\n",
    "\n",
    "# FIXME cannot really apply scaler_ytest on the predictions\n",
    "# predictions.loc[:] = scaler_ytest.inverse_transform(predictions).astype(int)\n",
    "\n",
    "predictions.loc['sj'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(city, mod1[city].evaluate(xtest_roll.loc[city], ytest_roll.loc[city, 'is_epidemic'], batch_size=batch_size)) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    df_plot = ytest_roll[['is_epidemic']].merge(predictions, left_index=True, right_index=True, suffixes=['_actual','_predicted'])\n",
    "    df_plot.loc[city].plot(figsize=(20,5))\n",
    "    plt.title(city)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100,\n",
    "              input_shape=(lahead, len(selected_features)),\n",
    "              batch_size=batch_size,\n",
    "              activation='linear'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Lambda(lambda x: x*10)) # TODO x*30 caused the re-fit on complete dataset to blow up\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-fit on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_retrain = df_all['features_train'][selected_features]\n",
    "y_retrain = df_all['labels_train'][['total_cases']]\n",
    "\n",
    "xretrain_scaled, scaler_xretrain = my_scale(x_retrain)\n",
    "# yretrain_scaled, scaler_yretrain = my_scale(y_retrain)\n",
    "yretrain_scaled = y_retrain\n",
    "scaler_yretrain = None\n",
    "\n",
    "xretrain_roll = xretrain_scaled.groupby(level='city').apply(stride_group)\n",
    "yretrain_roll = (yretrain_scaled\n",
    "                 .groupby(level='city', as_index=False)\n",
    "                 .apply(lambda group: group.iloc[lahead:])\n",
    "                 .reset_index(level=0, drop=True)\n",
    "                )\n",
    "\n",
    "for city in ['sj','iq']:\n",
    "    xretrain_roll.loc[city] = xretrain_roll.loc[city][(xretrain_roll.loc[city].shape[0]%batch_size):]\n",
    "    \n",
    "yretrain_roll = my_truncate(yretrain_roll)\n",
    "\n",
    "mod1 = {}\n",
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    mod1[city] = create_model()\n",
    "    mod1[city].summary()\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod1[city].fit(\n",
    "             xretrain_roll.loc[city],\n",
    "             yretrain_roll.loc[city],\n",
    "             batch_size=batch_size,\n",
    "             epochs=1000,\n",
    "             verbose=0,\n",
    "             #validation_data=None,\n",
    "             shuffle=False)\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    #plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title(city)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['submission'].loc['sj'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_submit = (df_all['features_test']\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.iloc[n_diff:])\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "          )\n",
    "xsubmit_scaled, scaler_xsubmit = my_scale(x_submit)\n",
    "xsubmit_roll = xsubmit_scaled.groupby(level='city').apply(stride_group)\n",
    "\n",
    "\n",
    "for city in ['sj','iq']:\n",
    "    xsubmit_roll.loc[city] = xsubmit_roll.loc[city][(xsubmit_roll.loc[city].shape[0]%batch_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (df_all['submission'][['total_cases']]\n",
    "               .groupby(level='city', as_index=False)\n",
    "               .apply(lambda group: group.iloc[(lahead+n_diff+1):])\n",
    "               .reset_index(level=0, drop=True)\n",
    "               .copy()\n",
    "               *0\n",
    "              ).astype('int')\n",
    "\n",
    "def my_predict(city):\n",
    "    np_pred = mod1[city].predict(xsubmit_roll.loc[city], batch_size=batch_size)\n",
    "    d1 = predictions.loc[city].shape[0]\n",
    "    d2 = xsubmit_roll.loc[city].shape[0]\n",
    "    return np.concatenate([np.zeros((d1-d2,1)), np_pred], axis=0)\n",
    "\n",
    "predictions.loc['sj', 'total_cases'] = my_predict('sj')\n",
    "predictions.loc['iq', 'total_cases'] = my_predict('iq')\n",
    "\n",
    "# FIXME cannot really apply scaler_ytest on the predictions\n",
    "#predictions.loc[:] = scaler_ytest.inverse_transform(predictions).astype(int)\n",
    "predictions['total_cases'] = predictions['total_cases'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df_all['submission'].copy()\n",
    "# TODO if this matches indeces properly, review the complicated merge in 3.1\n",
    "submit['total_cases'] = predictions\n",
    "submit = submit.fillna(value=0)\n",
    "submit['total_cases'] = submit['total_cases'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    plt.plot(submit.loc[city, 'total_cases'].values, label=city)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import make_submission"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "make_submission(submit.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

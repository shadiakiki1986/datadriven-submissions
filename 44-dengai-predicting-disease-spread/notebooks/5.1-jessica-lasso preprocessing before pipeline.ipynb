{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "- deseason features (from statsmodels.tsa and notebook 3.3.1)\n",
    "- [lasso linear](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "  - good at filtering out features when there are many of them\n",
    "- either lasso directly on target, or just use lasso for feature reduction and use another model like RF or OLS\n",
    "- append t-1 and t-2\n",
    "- use polynomial interactions of degree=3 but without t-1/t-2 or degree=2 with them (with/without for memory purposes)\n",
    "- use log10 transformation on target for training\n",
    "\n",
    "TODO\n",
    "- correlation matrix like in the [benchmark](https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb)\n",
    "- check [GaussianProcessRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)\n",
    "- check [related projects](http://scikit-learn.org/stable/related_projects.html) that handle sequences more properly (check `n_back` variable)\n",
    "  - [seqlearn](https://github.com/larsmans/seqlearn)\n",
    "  - [hmmlearn](http://hmmlearn.readthedocs.io/en/latest/auto_examples/plot_hmm_stock_analysis.html#sphx-glr-auto-examples-plot-hmm-stock-analysis-py)\n",
    "\n",
    "Result\n",
    "- the submission has one epidemic for SJ and none for IQ\n",
    "- having t-1 and t-2 appended doesn't help identify more epidemics\n",
    "- having polynomial degree = 3 also\n",
    "- keeping trend and residual from deseasoning is good\n",
    "- RF gives a smoother \"bump\", albeit smaller amplitude\n",
    "- t-0 .. t-10 without polynomial and RF yields larger amplitude\n",
    "- lasso cross-validation doesn't converge within 10k iterations\n",
    "\n",
    "Submissions\n",
    "- 5.1A .. lasso + RF .. score 30\n",
    "- 5.1B .. lasso + linear .. score 29\n",
    "- 5.1C .. lasso + linear with t-0..t-50 .. score 38\n",
    "- 5.1D .. lasso + RF with t-0..t-50 .. did not submit since no epidemics detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso, LinearRegression, LassoCV\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#import statsmodels.api as sm\n",
    "#import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import load_raw\n",
    "\n",
    "df_all = load_raw()\n",
    "\n",
    "# replace with 0.2 output\n",
    "# df_all['labels_train'] = pd.read_pickle('data/processed/is_epidemic.pkl')\n",
    "\n",
    "[(x, df_all[x].shape) for x in df_all.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['features_train'].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['labels_train'].head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gather all data into a single dataframe\n",
    "\n",
    "With this, deseasoning loses 52 points only once, instead of 3 times (train, test, submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['features_train']['submit'] = False\n",
    "df_all['features_test' ]['submit'] = True\n",
    "\n",
    "df_feat_1 = pd.concat([df_all['features_train'], df_all['features_test'], ], axis=0)\n",
    "df_targ   = pd.concat([df_all['labels_train'], df_all['submission'], ], axis=0)\n",
    "# df_one = pd.concat([df_feat, df_targ[['total_cases']]], axis=1)\n",
    "\n",
    "df_meta = df_feat_1[['submit']]\n",
    "del df_feat_1['submit']\n",
    "del df_feat_1['year']\n",
    "# del df_feat_1['weekofyear']\n",
    "\n",
    "df_feat_1.shape, df_targ.shape, df_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_one[df_one['total_cases']!=0].groupby('city').head(n=2)\n",
    "df_feat_1.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_targ.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.groupby('city').tail(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fillna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_1 = df_feat_1.groupby('city').apply(lambda group: group.fillna(method='ffill'))\n",
    "assert ~(pd.isnull(df_feat_1).any().any())\n",
    "print(df_feat_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define custom model for deseasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.utils import check_array\n",
    "\n",
    "\n",
    "class DeSeason(BaseEstimator):\n",
    "    def __init__(self, freq):\n",
    "        self.freq = freq\n",
    "        \n",
    "    def fit(self):\n",
    "        pass\n",
    "    \n",
    "    def transform(self, df_in):\n",
    "        return self.fit_transform(df_in)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        X = check_array(X)\n",
    "        df_interim = []\n",
    "        for jjj in range(X.shape[1]):\n",
    "            res0 = X[:,jjj]\n",
    "            res1 = res0 - res0.mean(axis=0)\n",
    "            res2 = seasonal_decompose(res1, freq=self.freq, two_sided=False)\n",
    "            res2 = pd.DataFrame({\n",
    "                #'original': res0,\n",
    "                'trend': res2.trend, \n",
    "                # FIXME # 'seasonal': res2.seasonal, \n",
    "                'resid': res2.resid\n",
    "                # 'chosen': res2.trend + res2.resid\n",
    "            })\n",
    "\n",
    "            # FIXME # res2['original'] = res0\n",
    "            res2 = res2.rename(columns={\n",
    "                #'original': \"%s_original\"%jjj,\n",
    "                'trend': \"%s_trend\"%jjj,\n",
    "                #'seasonal': \"%s_seasonal\"%jjj,\n",
    "                'resid': \"%s_resid\"%jjj,\n",
    "                #'chosen': \"%s_deseason\"%jjj,\n",
    "            })\n",
    "            df_interim.append(res2)\n",
    "            \n",
    "        # aggregate\n",
    "        df_interim = pd.concat(df_interim, axis=1)\n",
    "            \n",
    "        # MOVE SEQUENCING till after polynomial interactions\n",
    "        # break into t and t-1 and t-2\n",
    "        # col_new = lambda k: {x: \"%s_t%s\"%(x,k) for x in df_interim.columns}\n",
    "        # df_tm = []\n",
    "        # for m in range(20):\n",
    "        #    df_tm.append(df_interim.shift(m).rename(columns=col_new(m)))\n",
    "        #    \n",
    "        #df_interim = pd.concat(df_tm, axis=1)\n",
    "        \n",
    "        # fillna\n",
    "        df_interim = df_interim.fillna(value=0)\n",
    "\n",
    "        return df_interim\n",
    "    \n",
    "# test\n",
    "mdl = DeSeason(freq=2)\n",
    "df_in = np.array([\n",
    "    [1.0,2.0,3.0],[4.0,5.0,6.0],\n",
    "    [1.1,2.0,3.0],[4.1,5.0,6.0],\n",
    "    [1.2,2.0,3.0],[4.2,5.0,6.0],\n",
    "    [1.3,2.0,3.0],[4.3,5.0,6.0],\n",
    "])\n",
    "df_out = mdl.fit_transform(df_in)\n",
    "df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "deseason + min/max + polynomial interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_preprocess():\n",
    "    m0 = DeSeason(freq=52)\n",
    "    \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "    m1 = MinMaxScaler(feature_range=(0, 10))\n",
    "    \n",
    "    # FIXME disabling because lasso for iq at alpha=.1 or .5 was too slow\n",
    "    m2 = PolynomialFeatures(degree=2) # FIXME degree=3 runs out of memory with t-1 and t-2\n",
    "    \n",
    "    model = Pipeline([\n",
    "        ('deseason', m0),\n",
    "        ('scaler', m1),\n",
    "        ('poly', m2),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "mod0 = {}\n",
    "df_feat_2 = []\n",
    "for city in ['sj','iq']:\n",
    "    mod0[city] = create_preprocess()\n",
    "    df_temp = mod0[city].fit_transform(X = df_feat_1.loc[city], y = None)\n",
    "    df_temp = pd.DataFrame(\n",
    "        df_temp, \n",
    "        columns=mod0[city].named_steps['poly'].get_feature_names(), \n",
    "        index=df_feat_1.loc[city].index\n",
    "    )\n",
    "    df_temp['city'] = city\n",
    "    df_feat_2.append(df_temp.reset_index().set_index(['city','week_start_date']))\n",
    "    \n",
    "df_feat_2 = pd.concat(df_feat_2, axis=0)\n",
    "df_feat_2 = df_feat_2.loc[df_meta.index] # re-index as original\n",
    "\n",
    "df_feat_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build sequences\n",
    "t-0, t-1, t-2, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_back = 50\n",
    "\n",
    "col_new = lambda k: {x: \"%s_t%s\"%(x,k) for x in df_feat_2.columns}\n",
    "df_tm = []\n",
    "for m in range(n_back):\n",
    "    print('sequence %s'%m)\n",
    "    df_tm.append(df_feat_2.shift(m).rename(columns=col_new(m)))\n",
    "\n",
    "df_feat_2 = pd.concat(df_tm, axis=1)\n",
    "\n",
    "# fillna\n",
    "df_feat_2 = df_feat_2.fillna(value=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_2.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## choose features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected from\n",
    "# https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb\n",
    "#selected_features = ['reanalysis_specific_humidity_g_per_kg', \n",
    "#                 'reanalysis_dew_point_temp_k', \n",
    "#                 'station_avg_temp_c', \n",
    "#                 'station_min_temp_c']\n",
    "\n",
    "# all features\n",
    "# selected_features = df_all['features_train'].columns\n",
    "selected_features = df_feat_2.columns\n",
    "\n",
    "# without year and weekofyear\n",
    "# selected_features = np.array(list(set(df_all['features_train'].columns) - set(['year', 'weekofyear'])))\n",
    "\n",
    "# check no missing\n",
    "# assert len(set(selected_features) - set(df_all['features_train'].columns))==0\n",
    "\n",
    "#################################\n",
    "\n",
    "# all original/trend/seasonal features\n",
    "# selected_features = df_train.columns\n",
    "\n",
    "# only trend + weekofyear\n",
    "# import numpy as np\n",
    "# selected_features = np.array([x for x in df_train.columns if x.endswith('_trend')])# or x=='weekofyear'])\n",
    "\n",
    "#################\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_feat_2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for c in selected_features[:20]:\n",
    "    for city in ['sj','iq']:\n",
    "        df_train[c].loc[city].plot(figsize=(20,3), label=city)\n",
    "    plt.legend()\n",
    "    plt.title(c)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# split per city\n",
    "x_train = (#df_all['features_train']\n",
    "           df_train.loc[~df_meta['submit']]\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.head(n=group.shape[0]*7//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          # FIXME for no split, comment \"apply\" and \"reset above\", and uncomment \"apply\" below\n",
    "          #.apply(lambda group: group)\n",
    "          [selected_features]\n",
    "          )\n",
    "x_test = (#df_all['features_train']\n",
    "          df_train.loc[~df_meta['submit']]\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "         )\n",
    "y_train = ( #df_all['labels_train']\n",
    "            #df_all['labels_train'].loc[df_train.index]\n",
    "            df_targ.loc[~df_meta['submit']]\n",
    "          .groupby('city', as_index=False)\n",
    "           .apply(lambda group: group.head(n=group.shape[0]*7//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          # FIXME for no split, comment \"apply\" and \"reset above\", and uncomment \"apply\" below\n",
    "          # .apply(lambda group: group)\n",
    "          ['total_cases']\n",
    "          # ['is_epidemic'].astype('int')\n",
    "         )\n",
    "y_test = ( #df_all['labels_train']\n",
    "            #df_all['labels_train'].loc[df_train.index]\n",
    "            df_targ.loc[~df_meta['submit']]\n",
    "          .groupby('city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          ['total_cases']\n",
    "          # ['is_epidemic'].astype('int')\n",
    "         )\n",
    "\n",
    "y_train = 100*np.log10(y_train+1)\n",
    "y_test = 100*np.log10(y_test+1)\n",
    "\n",
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train.reset_index()['city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.groupby('city').describe()#tail(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.groupby('city').tail(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.groupby('city').tail(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model(alpha):\n",
    "    # return RandomForestRegressor(n_estimators=100, min_samples_split=5, min_samples_leaf=3)\n",
    "    # return RandomForestClassifier(n_estimators=100, min_samples_split=5, min_samples_leaf=3)\n",
    "    # return Lasso(alpha=1., normalize=True)\n",
    "    \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso\n",
    "    # max_iter default is 1k, but with t-1/t-2 and polynomials degree = 2, didnt converge\n",
    "    m31 = Lasso(alpha=alpha, normalize=False, positive=True, max_iter=10000)\n",
    "    \n",
    "    # use cross-validation: causes lasso not to converge within 10k iterations\n",
    "    # m31 = LassoCV(normalize=False, positive=False, max_iter=10000)\n",
    "    \n",
    "    # http://scikit-learn.org/stable/modules/feature_selection.html#l1-based-feature-selection\n",
    "    m32 = SelectFromModel(m31, prefit=False)\n",
    "    # m33 = RandomForestRegressor(n_estimators=100, min_samples_split=5, min_samples_leaf=3)\n",
    "    m33 = RandomForestRegressor(n_estimators=500, min_samples_split=15, min_samples_leaf=13)\n",
    "    \n",
    "    # m33 = LinearRegression(fit_intercept=False) # already appended intercept with polynomial\n",
    "    \n",
    "    model = Pipeline([\n",
    "        #('regressor', m31),\n",
    "        ('reducer', m32),\n",
    "        ('regressor', m33),\n",
    "    ])\n",
    "    # model.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
    "    return model\n",
    "\n",
    "lasso_settings = [\n",
    "    ('sj',1.),\n",
    "    # FIXME had .1 for iq when worked with Jessica\n",
    "    # Too slow with t-1..t-10\n",
    "    # Bumped up to speed up convergence\n",
    "    ('iq',.1),\n",
    "]\n",
    "\n",
    "mod1 = {}\n",
    "for city, alpha in lasso_settings:\n",
    "    # if city=='sj': continue\n",
    "    print(time.ctime(), city, 'fit start')\n",
    "    mod1[city] = create_model(alpha=alpha)\n",
    "    mod1[city].fit(X = x_train.loc[city], y = y_train.loc[city])\n",
    "    print(time.ctime(), city, 'fit end')\n",
    "    \n",
    "mod1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check feature importances"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(selected_features), len(mod1['sj'].named_steps['regressor'].coef_), len(mod1['iq'].named_steps['regressor'].coef_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "df_coef = pd.DataFrame({\n",
    "    'f': selected_features, # \n",
    "    #'f': mod1['sj'].named_steps['poly'].get_feature_names(), # with polynomial\n",
    "    'sj1': mod1['sj'].named_steps['regressor'].coef_, # lasso\n",
    "    # 'sj1': mod1_sj.named_steps['regressor'].feature_importances_, # RF\n",
    "    #'sj2': abs(mod1_sj.named_steps['regressor'].coef_),\n",
    "    'iq1': mod1['iq'].named_steps['regressor'].coef_, # lasso\n",
    "    # 'iq1': mod1_iq.named_steps['regressor'].feature_importances_, # RF\n",
    "    #'iq2': abs(mod1_iq.named_steps['regressor'].coef_),\n",
    "}).set_index('f')\n",
    "# .sort_values('sj2', ascending=False)\n",
    "df_coef[(abs(df_coef['iq1'])>.01) | (abs(df_coef['sj1'])>.01)] # lasso\n",
    "# df_coef[(abs(df_coef['iq1'])>.02) | (abs(df_coef['sj1'])>.02)] # RF\n",
    "# df_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features[[1,10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on train to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int since we know the label is integer\n",
    "predictions = (y_train.copy()*0).astype('int')\n",
    "\n",
    "for city in ['sj','iq']:\n",
    "    predictions.loc[city] = mod1[city].predict(x_train.loc[city])\n",
    "    #predictions = 10**predictions.astype('int')\n",
    "\n",
    "for city in ['sj', 'iq']:\n",
    "    plt.plot(y_train.loc[city], label='actual')\n",
    "    plt.plot(predictions.loc[city], label='predicted')\n",
    "    plt.title(city)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int since we know the label is integer\n",
    "predictions = (y_test.copy()*0).astype('int')\n",
    "\n",
    "for city in ['sj','iq']:\n",
    "    predictions.loc[city] = mod1[city].predict(x_test.loc[city])#.astype(int)\n",
    "    \n",
    "    # using sj model for iq\n",
    "    # predictions.loc[city] = mod1['sj'].predict(x_test.loc[city])#.astype(int)\n",
    "\n",
    "#predictions = (10**predictions).astype('int')\n",
    "# predictions.loc['sj'].head()\n",
    "\n",
    "for city in ['sj', 'iq']:\n",
    "    plt.plot(y_test.loc[city], label='actual')\n",
    "    plt.plot(predictions.loc[city], label='predicted')\n",
    "    plt.title(city)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'sj', mod1['sj'].score(x_test.loc['sj'], y_test.loc['sj']), 'iq', mod1['iq'].score(x_test.loc['iq'], y_test.loc['iq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-fit on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_2.shape, df_meta['submit'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_2.loc['sj'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.loc['sj'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_retrain = df_feat_2[selected_features][~df_meta['submit']]\n",
    "y_retrain = df_targ['total_cases'][~df_meta['submit']]\n",
    "y_retrain = 100*np.log10(y_retrain+1)\n",
    "\n",
    "mod2 = {}\n",
    "for city, alpha in lasso_settings:\n",
    "    print(time.ctime(), city, 'fit start')\n",
    "    mod2[city] = create_model(alpha=alpha)\n",
    "    mod2[city].fit(X = x_retrain.loc[city], y = y_retrain.loc[city])\n",
    "    print(time.ctime(), city, 'fit start')\n",
    "    \n",
    "mod2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_feat_2[df_meta['submit']]\n",
    "\n",
    "df_test.shape, df_targ['total_cases'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'weekofyear' in df_test.columns, 'weekofyear' in df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_2.shape, df_meta.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to int since we know the label is integer\n",
    "predictions = (df_all['submission'][['total_cases']]*0).astype('int')\n",
    "\n",
    "p1 = mod2['sj'].predict(df_test.loc['sj', selected_features])\n",
    "p1 = pd.DataFrame({'pred': p1, 'city': 'sj', 'week_start_date': df_test.loc['sj'].index})\n",
    "p2 = mod2['iq'].predict(df_test.loc['iq', selected_features])\n",
    "p2 = pd.DataFrame({'pred': p2, 'city': 'iq', 'week_start_date': df_test.loc['iq'].index})\n",
    "\n",
    "p3 = pd.concat([p1,p2], axis=0)\n",
    "p3 = p3.set_index(['city', 'week_start_date'])\n",
    "\n",
    "predictions = predictions.merge(p3, left_index=True, right_index=True, how='left').fillna(value=0)\n",
    "# predictions['pred'] = 10**predictions['pred'].astype('int')\n",
    "predictions['total_cases'] = predictions['pred']\n",
    "del predictions['pred']\n",
    "\n",
    "# postprocess to match with original format\n",
    "predictions['total_cases'] = ((10**((predictions['total_cases']/100).clip(upper=200000)))-1).astype('int')\n",
    "\n",
    "predictions.head(n=60).tail(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df_all['submission'].copy()\n",
    "# TODO Will this match indeces properly?\n",
    "# submit['total_cases'] = predictions\n",
    "\n",
    "del submit['total_cases']\n",
    "\n",
    "submit = submit.merge(\n",
    "    predictions,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "submit['total_cases'] = submit['total_cases'].fillna(value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    submit.loc[city, 'total_cases'].plot(figsize=(20,3), label=city)\n",
    "        \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import make_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(submit.reset_index())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

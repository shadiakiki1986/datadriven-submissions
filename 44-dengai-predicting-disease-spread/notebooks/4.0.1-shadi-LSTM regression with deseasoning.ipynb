{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change from 4.0\n",
    "\n",
    "- deseason before LSTM like notebook 5.1\n",
    "\n",
    "For reference, check https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb\n",
    "\n",
    "\n",
    "Submissions\n",
    "\n",
    "- 4.0.1A LSTM with deseasoning on all features, no polynomial features except intercept\n",
    "- 4.0.1B ditto with feature subset from notebook 3.0\n",
    "  - ditto from notebook 3.0.1 ... did not predict epidemic end of 2010 .. not submitting\n",
    "- 4.0.1C lahead=60, batchsize=64, feature subset from notebook 3.0.1\n",
    "- 4.0.1D ditto with all features\n",
    "  - without log10, end-of-2010 epidemic lasts a bit longer into 2011\n",
    "  - with log10(target+1)\n",
    "  - dropped `*_resid` for all features + kept `weekofyear_original` among `weekofyear_*` features\n",
    "  - submit score=24\n",
    "- 4.0.1E ditto with lstm(100) instead of lstm(10)\n",
    "  - submission score = 25\n",
    "  \n",
    "\n",
    "TODO\n",
    "- polynomial features degree > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import load_raw\n",
    "\n",
    "df_all = load_raw()\n",
    "\n",
    "[(x, df_all[x].shape) for x in df_all.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load 5.1 preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cell to execute (safety check)\n",
    "df_targ = pd.read_pickle('data/processed/5.1B-df_targ.pkl')\n",
    "df_feat_2 = pd.read_pickle('data/processed/5.1B-df_feat_2.pkl')\n",
    "df_meta = pd.read_pickle('data/processed/5.1B-df_meta.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_2.shape, df_targ.shape, df_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_feat_2.columns), len(df_all['features_train'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feat_2.columns, df_all['features_train'].columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def build_colnames(columns):\n",
    "    new_cols = []\n",
    "    new_cols = [[\"%s_resid\"%x, \"%s_trend\"%x] for x in columns if x!='year']\n",
    "    new_cols = [item for sublist in new_cols for item in sublist]\n",
    "    new_cols = {\"x%i\"%i: new_cols[i] for i in range(len(new_cols))}\n",
    "    new_cols.update({'1': 'intercept'})\n",
    "    return new_cols\n",
    "\n",
    "new_cols = build_colnames(df_all['features_train'].columns)\n",
    "\n",
    "# in case of polynomial features, need this too\n",
    "new_cols = {x: x.replace(x, new_cols[x]) for x in df_feat_2.columns}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_feat_2 = df_feat_2.rename(columns=new_cols)\n",
    "df_feat_2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features selected from\n",
    "# https://github.com/drivendata/benchmarks/blob/master/dengue-benchmark-statsmodels.ipynb\n",
    "#selected_features = ['reanalysis_specific_humidity_g_per_kg', \n",
    "#                 'reanalysis_dew_point_temp_k', \n",
    "#                 'station_avg_temp_c', \n",
    "#                 'station_min_temp_c']\n",
    "\n",
    "# all features\n",
    "# selected_features = df_all['features_train'].columns\n",
    "# selected_features = df_feat_2.columns\n",
    "\n",
    "# all that make sense\n",
    "selected_features = [x for x in df_feat_2.columns\n",
    "                     if (x.endswith('_trend') and not x.startswith('weekofyear'))\n",
    "                    or x=='weekofyear_original']\n",
    "\n",
    "# from RF feature importances (notebook 3.0)\n",
    "#selected_features = [\n",
    "#    'ndvi_sw',\n",
    "#    'ndvi_se',\n",
    "#    'reanalysis_dew_point_temp_k',\n",
    "#    'reanalysis_specific_humidity_g_per_kg',\n",
    "#    'station_max_temp_c',\n",
    "#    'weekofyear',\n",
    "#]\n",
    "#selected_features = list(build_colnames(selected_features).values())\n",
    "\n",
    "# from RF with diff (notebook 3.0.1)\n",
    "#selected_features = [\n",
    "#       'ndvi_sw',\n",
    "#       'ndvi_se',\n",
    "#       'reanalysis_avg_temp_k', # missing in 3.0\n",
    "#       'reanalysis_dew_point_temp_k',\n",
    "#       'reanalysis_specific_humidity_g_per_kg',\n",
    "#       'station_avg_temp_c', # missing in 3.0\n",
    "#       'station_max_temp_c',\n",
    "#       'weekofyear',\n",
    "#]\n",
    "#selected_features = list(build_colnames(selected_features).values())\n",
    "\n",
    "assert len(set(selected_features) - set(df_feat_2.columns))==0\n",
    "\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_feat_2.loc[~df_meta['submit']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note avoiding class bias\n",
    "x_train = (#df_all['features_train']\n",
    "            df_train\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.head(n=group.shape[0]*7//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "          )\n",
    "x_test = (#df_all['features_train']\n",
    "            df_train\n",
    "          .groupby(level='city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [selected_features]\n",
    "         )\n",
    "y_train = (#df_all['labels_train']\n",
    "            df_targ.loc[~df_meta['submit']]\n",
    "          .groupby('city', as_index=False)\n",
    "          .apply(lambda group: group.head(n=group.shape[0]*7//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [['total_cases']]\n",
    "         )\n",
    "y_test = (#df_all['labels_train']        \n",
    "            df_targ.loc[~df_meta['submit']]\n",
    "          .groupby('city', as_index=False)\n",
    "          .apply(lambda group: group.tail(n=group.shape[0]*1//8))\n",
    "          .reset_index(level=0, drop=True)\n",
    "          [['total_cases']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.groupby('city').head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train.reset_index()['city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log target"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train = np.log10(y_train+1)\n",
    "y_test = np.log10(y_test+1)\n",
    "\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize data to [-1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_scaled = x_train\n",
    "xtest_scaled = x_test\n",
    "ytrain_scaled = y_train\n",
    "ytest_scaled = y_test\n",
    "\n",
    "xtrain_scaled.shape, xtest_scaled.shape, ytrain_scaled.shape, ytest_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lahead = 60 # actually bad name .. should be look_back\n",
    "batch_size = 64 # smaller batches lead to less loss of data when truncating non-multiples of batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create rolling windows for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stride_group(group, lahead):\n",
    "    \"\"\"\n",
    "    https://pyfiddle.io/fiddle/a14865cf-38c9-48d6-b90a-f5bedc7a5b6e/\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i in range(lahead):\n",
    "        out.append(group.shift(i).values)\n",
    "        \n",
    "    out = np.stack(out, axis=2)[(lahead-1):, :, :] # drop first lahead\n",
    "    out = np.swapaxes(out, 1, 2)\n",
    "    out = np.flip(out, axis=1) # so that the index=0 is the oldest, and index=4 is latest\n",
    "    return out\n",
    "\n",
    "df_in = pd.DataFrame({'A': [1,2,3,4], 'B': [5,6,7,8]})\n",
    "df_out = stride_group(df_in, 2)\n",
    "\n",
    "print(df_in)\n",
    "print(df_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride_group_2 = lambda x: stride_group(x, lahead=lahead)    \n",
    "xtrain_roll = xtrain_scaled.groupby(level='city').apply(stride_group_2)\n",
    "xtest_roll  = xtest_scaled.groupby(level='city').apply(stride_group_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the target, drop first lahead points, without any rolling window\n",
    "ytrain_roll = (ytrain_scaled\n",
    "                .groupby(level='city', as_index=False)\n",
    "                .apply(lambda group: group.iloc[lahead:])\n",
    "                .reset_index(level=0, drop=True)\n",
    "                )\n",
    "ytest_roll = (ytest_scaled\n",
    "              .groupby(level='city', as_index=False)\n",
    "              .apply(lambda group: group.iloc[lahead:])\n",
    "              .reset_index(level=0, drop=True)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "[(city, [df.loc[city].shape for df in (xtrain_roll, xtest_roll, ytrain_roll, ytest_roll)]) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_roll.groupby('city').size(), ytest_roll.groupby('city').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WIP drop 1st x rows if they are not a multiple of batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    xtrain_roll.loc[city] = xtrain_roll.loc[city][(xtrain_roll.loc[city].shape[0]%batch_size):]\n",
    "    xtest_roll.loc[city] = xtest_roll.loc[city][(xtest_roll.loc[city].shape[0]%batch_size):]\n",
    "    \n",
    "def my_truncate(df):\n",
    "    return (df.groupby(level='city', as_index=False)\n",
    "              .apply(lambda group: group.tail(group.shape[0] - (group.shape[0]%batch_size)))\n",
    "              .reset_index(level=0, drop=True)\n",
    "            )\n",
    "\n",
    "ytrain_roll = my_truncate(ytrain_roll)\n",
    "ytest_roll = my_truncate(ytest_roll)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain_roll.groupby('city').size(), ytest_roll.groupby('city').size()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "[(city, [df.loc[city].shape for df in (xtrain_roll, xtest_roll, ytrain_roll, ytest_roll)]) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify data consistency between raw / scaled"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xtrain_scaled.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ytrain_scaled.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## verify data consistency between scaled / roll"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xtrain_scaled.loc['sj'].head(n=24+10).tail(n=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xtrain_roll.loc['sj'][:3,:,0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ytrain_roll.loc['sj'].head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://keras.io/layers/recurrent/#lstm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Lambda, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100,\n",
    "              input_shape=(lahead, len(selected_features)),\n",
    "              batch_size=batch_size,\n",
    "              activation='linear'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.add(Dropout(0.2))\n",
    "    # scaling in a lambda layer so that the previous layer doesn't have to learn the scale much\n",
    "    #model.add(Lambda(lambda x: x*10)) # TODO x*30 caused the re-fit on complete dataset to blow up\n",
    "    model.compile(loss='mae', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mod1 = {}\n",
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    mod1[city] = create_model()\n",
    "    mod1[city].summary()\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod1[city].fit(\n",
    "             xtrain_roll.loc[city],\n",
    "             ytrain_roll.loc[city],\n",
    "             batch_size=batch_size,\n",
    "             epochs=1000,\n",
    "             verbose=2,\n",
    "             validation_data=(xtest_roll.loc[city], ytest_roll.loc[city]),\n",
    "             shuffle=False)\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict on test set"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# cast to int since we know the label is integer\n",
    "predictions = (ytest_roll.copy()*0).astype('int')\n",
    "\n",
    "predictions.loc['sj'] = mod1['sj'].predict(xtest_roll.loc['sj'], batch_size=batch_size)#.astype(int)\n",
    "predictions.loc['iq'] = mod1['iq'].predict(xtest_roll.loc['iq'], batch_size=batch_size)#.astype(int)\n",
    "\n",
    "# FIXME cannot really apply scaler_ytest on the predictions\n",
    "# predictions.loc[:] = scaler_ytest.inverse_transform(predictions).astype(int)\n",
    "\n",
    "# predictions.loc['sj'].head()\n",
    "\n",
    "for city in ['sj', 'iq']:\n",
    "    df_plot = y_test.merge(predictions, left_index=True, right_index=True, suffixes=['_actual','_predicted'])\n",
    "    df_plot.loc[city].plot(figsize=(20,5))\n",
    "    plt.title(city)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[(city, mod1[city].evaluate(xtest_roll.loc[city], ytest_roll.loc[city], batch_size=batch_size)) for city in ['sj','iq']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-fit on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note avoiding class bias\n",
    "x_retrain = df_train[selected_features]\n",
    "y_retrain = df_targ.loc[~df_meta['submit']][['total_cases']]\n",
    "\n",
    "xretrain_scaled = x_retrain\n",
    "yretrain_scaled = y_retrain\n",
    "\n",
    "xretrain_roll = xretrain_scaled.groupby(level='city').apply(stride_group_2)\n",
    "yretrain_roll = (yretrain_scaled\n",
    "                 .groupby(level='city', as_index=False)\n",
    "                 .apply(lambda group: group.iloc[lahead:])\n",
    "                 .reset_index(level=0, drop=True)\n",
    "                )\n",
    "\n",
    "# drop lahead per city\n",
    "for city in ['sj','iq']:\n",
    "    xretrain_roll.loc[city] = xretrain_roll.loc[city][(xretrain_roll.loc[city].shape[0]%batch_size):]\n",
    "    \n",
    "yretrain_roll = my_truncate(yretrain_roll)\n",
    "\n",
    "y_retrain.groupby('city')['total_cases'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yretrain_roll = np.log10(yretrain_roll+1)\n",
    "\n",
    "for city in ['sj','iq']:\n",
    "    yretrain_roll.loc[city]['total_cases'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xretrain_roll.loc['sj'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xretrain_roll.loc['sj'][:,0,3], label='t0')\n",
    "plt.plot(xretrain_roll.loc['sj'][:,10,3], label='t10')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for city in ['sj', 'iq']:\n",
    "    print(city)\n",
    "    #if city=='sj': continue # FIXME fitting sj model\n",
    "    mod1[city] = create_model()\n",
    "    #if city=='iq': continue # FIXME skipping iq model\n",
    "    mod1[city].summary()\n",
    "    print(time.ctime(),'fit start')\n",
    "    history = mod1[city].fit(\n",
    "             xretrain_roll.loc[city],\n",
    "             yretrain_roll.loc[city],\n",
    "             batch_size=batch_size,\n",
    "             epochs=500, # 1000,\n",
    "             verbose=0,\n",
    "             #validation_data=None,\n",
    "             shuffle=False)\n",
    "    print(time.ctime(),'fit end')\n",
    "    \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    #plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.title(city)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set in submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['submission'].loc['sj'].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_submit = df_feat_2.loc[ df_meta['submit'], selected_features]\n",
    "xsubmit_scaled = x_submit\n",
    "xsubmit_roll = xsubmit_scaled.groupby(level='city').apply(stride_group_2)\n",
    "\n",
    "# drop non-batch_size multiple\n",
    "for city in ['sj','iq']:\n",
    "    xsubmit_roll.loc[city] = xsubmit_roll.loc[city][(xsubmit_roll.loc[city].shape[0]%batch_size):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (df_targ.loc[ df_meta['submit']].copy()*0).astype('int')[['total_cases']]\n",
    "\n",
    "def my_predict(city):\n",
    "    np_pred = mod1[city].predict(xsubmit_roll.loc[city], batch_size=batch_size)\n",
    "    d1 = predictions.loc[city].shape[0]\n",
    "    d2 = xsubmit_roll.loc[city].shape[0]\n",
    "    return np.concatenate([np.zeros((d1-d2,1)), np_pred], axis=0)\n",
    "\n",
    "predictions.loc['sj', 'total_cases'] = my_predict('sj')\n",
    "predictions.loc['iq', 'total_cases'] = my_predict('iq')\n",
    "\n",
    "# reverse log10 transform\n",
    "predictions['total_cases'] = ((10**((predictions['total_cases']).clip(upper=3)))-1)\n",
    "\n",
    "\n",
    "# FIXME cannot really apply scaler_ytest on the predictions\n",
    "#predictions.loc[:] = scaler_ytest.inverse_transform(predictions).astype(int)\n",
    "predictions['total_cases'] = predictions['total_cases'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupby('city').tail(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = df_all['submission'].copy()\n",
    "submit = submit.merge(predictions, how='left', left_index=True, right_index=True, suffixes=['_zero', ''])\n",
    "del submit['total_cases_zero']\n",
    "submit = submit.fillna(value=0)\n",
    "submit['total_cases'] = submit['total_cases'].astype('int')\n",
    "submit.groupby('city').tail(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in ['sj','iq']:\n",
    "    submit.loc[city, 'total_cases'].plot(figsize=(20,3), label=city)\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.build_features import make_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_submission(submit.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
